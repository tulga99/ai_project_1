{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41d234c2-b7bf-40f6-8f44-be6b165d1e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6c3e1ff8-5645-4c34-a48e-ab6eea9a6631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "    \n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_iter, lr, no_bias, verbose): \n",
    "        \n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.bias = no_bias\n",
    "        self.verbose = verbose\n",
    "     \n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "      \n",
    "\n",
    "    # 問題6（学習と推定）\n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "        \n",
    "        if self.bias == True:\n",
    "            bias = np.ones((X.shape[0], 1))\n",
    "            X = np.hstack((bias, X))\n",
    "            if X_val is not None:\n",
    "                bias = np.ones((X_val.shape[0], 1))\n",
    "                X_val = np.hstack((bias, X_val))\n",
    "            self.coef_ = np.random.rand(X.shape[1])\n",
    "            self.coef_ = self.coef_.reshape(X.shape[1], 1)\n",
    "    \n",
    "\n",
    "        for epoch in range(self.iter):\n",
    "            y_pred = self._linear_hypothesis(X)\n",
    "            self.loss[epoch] = np.mean((y-y_pred)**2)\n",
    "            \n",
    "            if X_val is not None:\n",
    "                pred_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[epoch] = np.mean((y_val-pred_val)**2)\n",
    "                \n",
    "            self.coef_ = self._gradient_descent(X, (y_pred-y))\n",
    "           \n",
    "            if self.verbose == True:\n",
    "                print('{}-th epoch train loss {}'.format(epoch, self.loss[epoch]))\n",
    "                if X_val is not None:\n",
    "                    print('{}-th epoch val loss {}'.format(epoch, self.val_loss[epoch] ))\n",
    "\n",
    "\n",
    "    # 問題1\n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        仮定関数の出力を計算する\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        Returns\n",
    "        -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形の仮定関数による推定結果\n",
    "        \"\"\"\n",
    "        pred = X @ self.coef_\n",
    "        \n",
    "        return pred\n",
    "\n",
    "    # 問題2\n",
    "    def _gradient_descent(self, X, error):\n",
    "\n",
    "        for i in range(X.shape[1]):\n",
    "            gradient = error*X[:, i]\n",
    "            self.coef_[i, :] = self.coef_[i, :] - self.lr * np.mean(gradient)\n",
    "\n",
    "        return self.coef_\n",
    "        \n",
    "\n",
    "    # 問題3\n",
    "    def predict(self, X):\n",
    "        if self.bias == True:\n",
    "            bias = np.ones(X.shape[0]).reshape(X.shape[0], 1)\n",
    "            X = np.hstack([bias, X])\n",
    "        pred_y = self._linear_hypothesis(X)\n",
    "        return pred_y\n",
    "\n",
    "    # 問題4\n",
    "    def _mse(self, y_pred, y):\n",
    "        \"\"\"\n",
    "        平均二乗誤差の計算\n",
    "        \"\"\"\n",
    "        mse = np.mean((y-y_pred)**2)\n",
    "        \n",
    "        return mse\n",
    "\n",
    "    # 問題5\n",
    "    def _loss_func(self, pred, y):\n",
    "        \"\"\"\n",
    "        損失関数\n",
    "        \"\"\"\n",
    "        loss = self._mse(pred, y)/2\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c5adcaa-528e-4e2e-9628-3f6c2e5cc225",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "dataset = pd.read_csv(\"../big_data/train.csv\")\n",
    "X = dataset.loc[:, ['GrLivArea', 'YearBuilt']]\n",
    "y = dataset.loc[:, ['SalePrice']]\n",
    "X = X.values\n",
    "X = MinMaxScaler().fit_transform(X)\n",
    "y = np.log(y.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3304252e-cb09-4622-a891-47c3eb880899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-th epoch train loss 110.66981804092384\n",
      "0-th epoch val loss 110.36941087720369\n",
      "1-th epoch train loss 107.21780544637006\n",
      "1-th epoch val loss 106.94594460571851\n",
      "2-th epoch train loss 103.87394759676323\n",
      "2-th epoch val loss 103.62945961795415\n",
      "3-th epoch train loss 100.63486391893956\n",
      "3-th epoch val loss 100.41661638420574\n",
      "4-th epoch train loss 97.4972793803407\n",
      "4-th epoch val loss 97.3041795655806\n",
      "5-th epoch train loss 94.45802119602928\n",
      "5-th epoch val loss 94.28901476420265\n",
      "6-th epoch train loss 91.51401563841785\n",
      "6-th epoch val loss 91.36808537476657\n",
      "7-th epoch train loss 88.66228494650882\n",
      "7-th epoch val loss 88.53844953428157\n",
      "8-th epoch train loss 85.89994433154132\n",
      "8-th epoch val loss 85.79725716694232\n",
      "9-th epoch train loss 83.22419907603877\n",
      "9-th epoch val loss 83.14174712116109\n",
      "10-th epoch train loss 80.63234172334379\n",
      "10-th epoch val loss 80.56924439588666\n",
      "11-th epoch train loss 78.1217493548189\n",
      "11-th epoch val loss 78.07715745342567\n",
      "12-th epoch train loss 75.6898809519785\n",
      "12-th epoch val loss 75.66297561606937\n",
      "13-th epoch train loss 73.33427484090353\n",
      "13-th epoch val loss 73.32426654391148\n",
      "14-th epoch train loss 71.05254621637273\n",
      "14-th epoch val loss 71.05867379132607\n",
      "15-th epoch train loss 68.84238474322395\n",
      "15-th epoch val loss 68.86391443965184\n",
      "16-th epoch train loss 66.70155223253728\n",
      "16-th epoch val loss 66.73777680370681\n",
      "17-th epoch train loss 64.62788039030644\n",
      "17-th epoch val loss 64.67811820983063\n",
      "18-th epoch train loss 62.61926863633711\n",
      "18-th epoch val loss 62.68286284322426\n",
      "19-th epoch train loss 60.67368199118279\n",
      "19-th epoch val loss 60.749999662425644\n",
      "20-th epoch train loss 58.78914902899563\n",
      "20-th epoch val loss 58.87758037882825\n",
      "21-th epoch train loss 56.963759894236645\n",
      "21-th epoch val loss 57.06371749921347\n",
      "22-th epoch train loss 55.19566438025373\n",
      "22-th epoch val loss 55.30658242933243\n",
      "23-th epoch train loss 53.483070067797776\n",
      "23-th epoch val loss 53.604403636632924\n",
      "24-th epoch train loss 51.824240521607585\n",
      "24-th epoch val loss 51.95546487028722\n",
      "25-th epoch train loss 50.2174935432525\n",
      "25-th epoch val loss 50.3581034367338\n",
      "26-th epoch train loss 48.661199478478046\n",
      "26-th epoch val loss 48.81070852900174\n",
      "27-th epoch train loss 47.153779577354925\n",
      "27-th epoch val loss 47.31171960814065\n",
      "28-th epoch train loss 45.693704405584185\n",
      "28-th epoch val loss 45.85962483513113\n",
      "29-th epoch train loss 44.27949230536327\n",
      "29-th epoch val loss 44.4529595517014\n",
      "30-th epoch train loss 42.909707904267194\n",
      "30-th epoch val loss 43.09030480852525\n",
      "31-th epoch train loss 41.58296067064703\n",
      "31-th epoch val loss 41.77028593932338\n",
      "32-th epoch train loss 40.29790351409528\n",
      "32-th epoch val loss 40.491571179436846\n",
      "33-th epoch train loss 39.05323142957208\n",
      "33-th epoch val loss 39.25287032748555\n",
      "34-th epoch train loss 37.84768018383079\n",
      "34-th epoch val loss 38.05293344876827\n",
      "35-th epoch train loss 36.68002504282358\n",
      "35-th epoch val loss 36.89054961910244\n",
      "36-th epoch train loss 35.54907953880872\n",
      "36-th epoch val loss 35.764545707842494\n",
      "37-th epoch train loss 34.453694275921485\n",
      "37-th epoch val loss 34.67378519885513\n",
      "38-th epoch train loss 33.392755773008865\n",
      "38-th epoch val loss 33.617167048267504\n",
      "39-th epoch train loss 32.36518534256581\n",
      "39-th epoch val loss 32.59362457784186\n",
      "40-th epoch train loss 31.36993800464709\n",
      "40-th epoch val loss 31.60212440286527\n",
      "41-th epoch train loss 30.406001434663754\n",
      "41-th epoch val loss 30.64166539347837\n",
      "42-th epoch train loss 29.472394944007373\n",
      "42-th epoch val loss 29.711277668400005\n",
      "43-th epoch train loss 28.56816849247811\n",
      "43-th epoch val loss 28.81002162003767\n",
      "44-th epoch train loss 27.69240173152466\n",
      "44-th epoch val loss 27.93698697000487\n",
      "45-th epoch train loss 26.844203077335006\n",
      "45-th epoch val loss 27.091291854097122\n",
      "46-th epoch train loss 26.022708812846837\n",
      "46-th epoch val loss 26.27208193580787\n",
      "47-th epoch train loss 25.227082217775756\n",
      "47-th epoch val loss 25.478529547494354\n",
      "48-th epoch train loss 24.456512725787128\n",
      "48-th epoch val loss 24.70983285833102\n",
      "49-th epoch train loss 23.710215107965123\n",
      "49-th epoch val loss 23.965215068215066\n",
      "50-th epoch train loss 22.987428681758594\n",
      "50-th epoch val loss 23.243923626814926\n",
      "51-th epoch train loss 22.287416544609155\n",
      "51-th epoch val loss 22.54522947697733\n",
      "52-th epoch train loss 21.609464831491525\n",
      "52-th epoch val loss 21.868426321733466\n",
      "53-th epoch train loss 20.952881995620363\n",
      "53-th epoch val loss 21.212829914168164\n",
      "54-th epoch train loss 20.316998111600835\n",
      "54-th epoch val loss 20.577777369439122\n",
      "55-th epoch train loss 19.701164200322978\n",
      "55-th epoch val loss 19.962626498255386\n",
      "56-th epoch train loss 19.104751574921515\n",
      "56-th epoch val loss 19.366755161145765\n",
      "57-th epoch train loss 18.527151207144072\n",
      "57-th epoch val loss 18.789560642868917\n",
      "58-th epoch train loss 17.967773113491223\n",
      "58-th epoch val loss 18.23045904633687\n",
      "59-th epoch train loss 17.426045760511514\n",
      "59-th epoch val loss 17.688884705443378\n",
      "60-th epoch train loss 16.90141548865412\n",
      "60-th epoch val loss 17.164289616207647\n",
      "61-th epoch train loss 16.393345954100088\n",
      "61-th epoch val loss 16.656142885662096\n",
      "62-th epoch train loss 15.901317588011484\n",
      "62-th epoch val loss 16.163930197930785\n",
      "63-th epoch train loss 15.424827072655003\n",
      "63-th epoch val loss 15.687153296962475\n",
      "64-th epoch train loss 14.963386833873784\n",
      "64-th epoch val loss 15.225329485398763\n",
      "65-th epoch train loss 14.516524549397296\n",
      "65-th epoch val loss 14.777991139074166\n",
      "66-th epoch train loss 14.083782672495413\n",
      "66-th epoch val loss 14.344685236660615\n",
      "67-th epoch train loss 13.664717970497861\n",
      "67-th epoch val loss 13.924972903983996\n",
      "68-th epoch train loss 13.25890107771544\n",
      "68-th epoch val loss 13.518428972555201\n",
      "69-th epoch train loss 12.865916062313676\n",
      "69-th epoch val loss 13.124641551872319\n",
      "70-th epoch train loss 12.485360006703708\n",
      "70-th epoch val loss 12.743211615064519\n",
      "71-th epoch train loss 12.116842601028669\n",
      "71-th epoch val loss 12.373752597461475\n",
      "72-th epoch train loss 11.759985749337122\n",
      "72-th epoch val loss 12.015890007685236\n",
      "73-th epoch train loss 11.41442318804769\n",
      "73-th epoch val loss 11.66926105087396\n",
      "74-th epoch train loss 11.079800116321493\n",
      "74-th epoch val loss 11.333514263659197\n",
      "75-th epoch train loss 10.755772837970895\n",
      "75-th epoch val loss 11.008309160530032\n",
      "76-th epoch train loss 10.442008414544642\n",
      "76-th epoch val loss 10.693315891229084\n",
      "77-th epoch train loss 10.1381843292408\n",
      "77-th epoch val loss 10.388214908836169\n",
      "78-th epoch train loss 9.843988161309598\n",
      "78-th epoch val loss 10.09269664820638\n",
      "79-th epoch train loss 9.559117270618993\n",
      "79-th epoch val loss 9.806461214439567\n",
      "80-th epoch train loss 9.28327849206589\n",
      "80-th epoch val loss 9.529218081068407\n",
      "81-th epoch train loss 9.016187839525822\n",
      "81-th epoch val loss 9.260685797661914\n",
      "82-th epoch train loss 8.757570219043567\n",
      "82-th epoch val loss 9.000591706550725\n",
      "83-th epoch train loss 8.507159150976351\n",
      "83-th epoch val loss 8.74867166838969\n",
      "84-th epoch train loss 8.264696500810334\n",
      "84-th epoch val loss 8.504669796282084\n",
      "85-th epoch train loss 8.02993221837984\n",
      "85-th epoch val loss 8.268338198198467\n",
      "86-th epoch train loss 7.80262408522708\n",
      "86-th epoch val loss 8.039436727431426\n",
      "87-th epoch train loss 7.582537469848493\n",
      "87-th epoch val loss 7.817732740835585\n",
      "88-th epoch train loss 7.369445090581559\n",
      "88-th epoch val loss 7.6030008646101\n",
      "89-th epoch train loss 7.163126785893758\n",
      "89-th epoch val loss 7.395022767388283\n",
      "90-th epoch train loss 6.963369291842694\n",
      "90-th epoch val loss 7.193586940406551\n",
      "91-th epoch train loss 6.769966026483676\n",
      "91-th epoch val loss 6.998488484531855\n",
      "92-th epoch train loss 6.58271688100795\n",
      "92-th epoch val loss 6.8095289039336215\n",
      "93-th epoch train loss 6.401428017401614\n",
      "93-th epoch val loss 6.6265159061930445\n",
      "94-th epoch train loss 6.2259116724217325\n",
      "94-th epoch val loss 6.449263208648863\n",
      "95-th epoch train loss 6.055985967692597\n",
      "95-th epoch val loss 6.2775903507851725\n",
      "96-th epoch train loss 5.891474725731072\n",
      "96-th epoch val loss 6.111322512472733\n",
      "97-th epoch train loss 5.732207291716152\n",
      "97-th epoch val loss 5.950290337881278\n",
      "98-th epoch train loss 5.578018360823396\n",
      "98-th epoch val loss 5.794329764885867\n",
      "99-th epoch train loss 5.428747810950646\n",
      "99-th epoch val loss 5.643281859795974\n",
      "100-th epoch train loss 5.284240540666799\n",
      "100-th epoch val loss 5.49699265724123\n",
      "101-th epoch train loss 5.144346312220647\n",
      "101-th epoch val loss 5.355313005053022\n",
      "102-th epoch train loss 5.008919599451898\n",
      "102-th epoch val loss 5.218098413986086\n",
      "103-th epoch train loss 4.8778194404513915\n",
      "103-th epoch val loss 5.085208912129136\n",
      "104-th epoch train loss 4.750909294822366\n",
      "104-th epoch val loss 4.956508903858274\n",
      "105-th epoch train loss 4.628056905399116\n",
      "105-th epoch val loss 4.831867033191456\n",
      "106-th epoch train loss 4.509134164284028\n",
      "106-th epoch val loss 4.711156051406764\n",
      "107-th epoch train loss 4.39401698306819\n",
      "107-th epoch val loss 4.594252688791468\n",
      "108-th epoch train loss 4.282585167105041\n",
      "108-th epoch val loss 4.481037530393016\n",
      "109-th epoch train loss 4.17472229371056\n",
      "109-th epoch val loss 4.3713948956471365\n",
      "110-th epoch train loss 4.070315594167498\n",
      "110-th epoch val loss 4.265212721762115\n",
      "111-th epoch train loss 3.9692558394148847\n",
      "111-th epoch val loss 4.162382450742053\n",
      "112-th epoch train loss 3.871437229307866\n",
      "112-th epoch val loss 4.0627989199356245\n",
      "113-th epoch train loss 3.776757285336391\n",
      "113-th epoch val loss 3.966360256000338\n",
      "114-th epoch train loss 3.6851167466948325\n",
      "114-th epoch val loss 3.8729677721757607\n",
      "115-th epoch train loss 3.59641946959797\n",
      "115-th epoch val loss 3.7825258687624927\n",
      "116-th epoch train loss 3.510572329741997\n",
      "116-th epoch val loss 3.694941936706892\n",
      "117-th epoch train loss 3.427485127812411\n",
      "117-th epoch val loss 3.6101262641946454\n",
      "118-th epoch train loss 3.3470704979436885\n",
      "118-th epoch val loss 3.5279919461593594\n",
      "119-th epoch train loss 3.269243819038623\n",
      "119-th epoch val loss 3.4484547966151973\n",
      "120-th epoch train loss 3.1939231288580645\n",
      "120-th epoch val loss 3.3714332637255096\n",
      "121-th epoch train loss 3.1210290407945993\n",
      "121-th epoch val loss 3.2968483475220567\n",
      "122-th epoch train loss 3.0504846632463996\n",
      "122-th epoch val loss 3.2246235201921856\n",
      "123-th epoch train loss 2.982215521510077\n",
      "123-th epoch val loss 3.1546846488538183\n",
      "124-th epoch train loss 2.9161494821139278\n",
      "124-th epoch val loss 3.0869599207406537\n",
      "125-th epoch train loss 2.8522166795153927\n",
      "125-th epoch val loss 3.021379770722401\n",
      "126-th epoch train loss 2.79034944508892\n",
      "126-th epoch val loss 2.957876811087175\n",
      "127-th epoch train loss 2.7304822383327547\n",
      "127-th epoch val loss 2.8963857635155024\n",
      "128-th epoch train loss 2.6725515802254036\n",
      "128-th epoch val loss 2.8368433931775754\n",
      "129-th epoch train loss 2.616495988664629\n",
      "129-th epoch val loss 2.779188444887476\n",
      "130-th epoch train loss 2.5622559159240095\n",
      "130-th epoch val loss 2.7233615812502387\n",
      "131-th epoch train loss 2.5097736880640644\n",
      "131-th epoch val loss 2.669305322739557\n",
      "132-th epoch train loss 2.4589934462369247\n",
      "132-th epoch val loss 2.616963989645908\n",
      "133-th epoch train loss 2.409861089825445\n",
      "133-th epoch val loss 2.5662836458367435\n",
      "134-th epoch train loss 2.362324221359485\n",
      "134-th epoch val loss 2.5172120442721972\n",
      "135-th epoch train loss 2.3163320931538873\n",
      "135-th epoch val loss 2.4696985742215727\n",
      "136-th epoch train loss 2.2718355556143894\n",
      "136-th epoch val loss 2.4236942101275116\n",
      "137-th epoch train loss 2.2287870071594127\n",
      "137-th epoch val loss 2.379151462066465\n",
      "138-th epoch train loss 2.187140345707255\n",
      "138-th epoch val loss 2.3360243277556414\n",
      "139-th epoch train loss 2.1468509216798393\n",
      "139-th epoch val loss 2.2942682460581882\n",
      "140-th epoch train loss 2.1078754924756633\n",
      "140-th epoch val loss 2.253840051939884\n",
      "141-th epoch train loss 2.070172178366063\n",
      "141-th epoch val loss 2.2146979328320144\n",
      "142-th epoch train loss 2.0337004197703648\n",
      "142-th epoch val loss 2.1768013863565883\n",
      "143-th epoch train loss 1.9984209358668694\n",
      "143-th epoch val loss 2.140111179371377\n",
      "144-th epoch train loss 1.964295684497953\n",
      "144-th epoch val loss 2.1045893082935914\n",
      "145-th epoch train loss 1.9312878233288835\n",
      "145-th epoch val loss 2.070198960662314\n",
      "146-th epoch train loss 1.8993616722211937\n",
      "146-th epoch val loss 2.036904477901021\n",
      "147-th epoch train loss 1.8684826767827003\n",
      "147-th epoch val loss 2.0046713192427617\n",
      "148-th epoch train loss 1.8386173730574067\n",
      "148-th epoch val loss 1.9734660267817044\n",
      "149-th epoch train loss 1.8097333533197106\n",
      "149-th epoch val loss 1.9432561916159132\n",
      "150-th epoch train loss 1.7817992329384242\n",
      "150-th epoch val loss 1.9140104210473015\n",
      "151-th epoch train loss 1.7547846182771905\n",
      "151-th epoch val loss 1.8856983068057735\n",
      "152-th epoch train loss 1.7286600755989427\n",
      "152-th epoch val loss 1.8582903942655988\n",
      "153-th epoch train loss 1.7033971009430373\n",
      "153-th epoch val loss 1.8317581526230542\n",
      "154-th epoch train loss 1.6789680909446916\n",
      "154-th epoch val loss 1.806073946005337\n",
      "155-th epoch train loss 1.6553463145672884\n",
      "155-th epoch val loss 1.7812110054816943\n",
      "156-th epoch train loss 1.6325058857190449\n",
      "156-th epoch val loss 1.7571434019486116\n",
      "157-th epoch train loss 1.6104217367264113\n",
      "157-th epoch val loss 1.7338460198617804\n",
      "158-th epoch train loss 1.5890695926374552\n",
      "158-th epoch val loss 1.7112945317884307\n",
      "159-th epoch train loss 1.5684259463292833\n",
      "159-th epoch val loss 1.6894653737544083\n",
      "160-th epoch train loss 1.5484680343944215\n",
      "160-th epoch val loss 1.6683357213612322\n",
      "161-th epoch train loss 1.5291738137817765\n",
      "161-th epoch val loss 1.6478834666490545\n",
      "162-th epoch train loss 1.5105219391686435\n",
      "162-th epoch val loss 1.6280871956822822\n",
      "163-th epoch train loss 1.4924917410409029\n",
      "163-th epoch val loss 1.6089261668352963\n",
      "164-th epoch train loss 1.4750632044592955\n",
      "164-th epoch val loss 1.5903802897564199\n",
      "165-th epoch train loss 1.4582169484903231\n",
      "165-th epoch val loss 1.5724301049889573\n",
      "166-th epoch train loss 1.4419342062810327\n",
      "166-th epoch val loss 1.5550567642288198\n",
      "167-th epoch train loss 1.426196805757555\n",
      "167-th epoch val loss 1.5382420111988482\n",
      "168-th epoch train loss 1.4109871509279055\n",
      "168-th epoch val loss 1.5219681631205981\n",
      "169-th epoch train loss 1.3962882037701823\n",
      "169-th epoch val loss 1.506218092764939\n",
      "170-th epoch train loss 1.3820834666878525\n",
      "170-th epoch val loss 1.4909752110633947\n",
      "171-th epoch train loss 1.3683569655144228\n",
      "171-th epoch val loss 1.4762234502627336\n",
      "172-th epoch train loss 1.3550932330503123\n",
      "172-th epoch val loss 1.4619472476058435\n",
      "173-th epoch train loss 1.3422772931153124\n",
      "173-th epoch val loss 1.4481315295224728\n",
      "174-th epoch train loss 1.3298946451005091\n",
      "174-th epoch val loss 1.4347616963139174\n",
      "175-th epoch train loss 1.3179312490040678\n",
      "175-th epoch val loss 1.4218236073162407\n",
      "176-th epoch train loss 1.306373510935748\n",
      "176-th epoch val loss 1.4093035665270794\n",
      "177-th epoch train loss 1.2952082690755125\n",
      "177-th epoch val loss 1.3971883086815844\n",
      "178-th epoch train loss 1.2844227800720287\n",
      "178-th epoch val loss 1.3854649857634507\n",
      "179-th epoch train loss 1.2740047058673132\n",
      "179-th epoch val loss 1.374121153937479\n",
      "180-th epoch train loss 1.2639421009342076\n",
      "180-th epoch val loss 1.3631447608904976\n",
      "181-th epoch train loss 1.254223399913772\n",
      "181-th epoch val loss 1.352524133567898\n",
      "182-th epoch train loss 1.2448374056401008\n",
      "182-th epoch val loss 1.3422479662934381\n",
      "183-th epoch train loss 1.235773277540449\n",
      "183-th epoch val loss 1.3323053092603454\n",
      "184-th epoch train loss 1.2270205203989302\n",
      "184-th epoch val loss 1.3226855573821266\n",
      "185-th epoch train loss 1.218568973472435\n",
      "185-th epoch val loss 1.3133784394918657\n",
      "186-th epoch train loss 1.2104087999477373\n",
      "186-th epoch val loss 1.3043740078791133\n",
      "187-th epoch train loss 1.2025304767291383\n",
      "187-th epoch val loss 1.2956626281538393\n",
      "188-th epoch train loss 1.1949247845463056\n",
      "188-th epoch val loss 1.2872349694272374\n",
      "189-th epoch train loss 1.1875827983722957\n",
      "189-th epoch val loss 1.279081994799487\n",
      "190-th epoch train loss 1.1804958781420607\n",
      "190-th epoch val loss 1.2711949521448829\n",
      "191-th epoch train loss 1.1736556597620469\n",
      "191-th epoch val loss 1.263565365185061\n",
      "192-th epoch train loss 1.1670540464017762\n",
      "192-th epoch val loss 1.2561850248413142\n",
      "193-th epoch train loss 1.1606832000585956\n",
      "193-th epoch val loss 1.2490459808572854\n",
      "194-th epoch train loss 1.1545355333870475\n",
      "194-th epoch val loss 1.242140533683603\n",
      "195-th epoch train loss 1.1486037017845905\n",
      "195-th epoch val loss 1.235461226616268\n",
      "196-th epoch train loss 1.1428805957256478\n",
      "196-th epoch val loss 1.2290008381808826\n",
      "197-th epoch train loss 1.137359333336213\n",
      "197-th epoch val loss 1.222752374755028\n",
      "198-th epoch train loss 1.1320332532014967\n",
      "198-th epoch val loss 1.2167090634213693\n",
      "199-th epoch train loss 1.126895907399315\n",
      "199-th epoch val loss 1.2108643450442715\n",
      "200-th epoch train loss 1.1219410547521649\n",
      "200-th epoch val loss 1.2052118675629577\n",
      "201-th epoch train loss 1.117162654291138\n",
      "201-th epoch val loss 1.1997454794944324\n",
      "202-th epoch train loss 1.1125548589250496\n",
      "202-th epoch val loss 1.1944592236396332\n",
      "203-th epoch train loss 1.1081120093083598\n",
      "203-th epoch val loss 1.1893473309864537\n",
      "204-th epoch train loss 1.1038286279016665\n",
      "204-th epoch val loss 1.1844042148034972\n",
      "205-th epoch train loss 1.099699413218742\n",
      "205-th epoch val loss 1.1796244649185963\n",
      "206-th epoch train loss 1.0957192342542823\n",
      "206-th epoch val loss 1.1750028421763317\n",
      "207-th epoch train loss 1.091883125086702\n",
      "207-th epoch val loss 1.170534273068961\n",
      "208-th epoch train loss 1.0881862796505106\n",
      "208-th epoch val loss 1.1662138445353403\n",
      "209-th epoch train loss 1.0846240466729433\n",
      "209-th epoch val loss 1.1620367989225815\n",
      "210-th epoch train loss 1.0811919247697281\n",
      "210-th epoch val loss 1.1579985291053825\n",
      "211-th epoch train loss 1.077885557694986\n",
      "211-th epoch val loss 1.1540945737580846\n",
      "212-th epoch train loss 1.0747007297404492\n",
      "212-th epoch val loss 1.150320612774694\n",
      "213-th epoch train loss 1.0716333612793265\n",
      "213-th epoch val loss 1.1466724628322527\n",
      "214-th epoch train loss 1.0686795044502766\n",
      "214-th epoch val loss 1.1431460730930671\n",
      "215-th epoch train loss 1.065835338977107\n",
      "215-th epoch val loss 1.1397375210414629\n",
      "216-th epoch train loss 1.0630971681199517\n",
      "216-th epoch val loss 1.1364430084508657\n",
      "217-th epoch train loss 1.0604614147538076\n",
      "217-th epoch val loss 1.1332588574771338\n",
      "218-th epoch train loss 1.0579246175704347\n",
      "218-th epoch val loss 1.1301815068741916\n",
      "219-th epoch train loss 1.0554834273997666\n",
      "219-th epoch val loss 1.127207508328156\n",
      "220-th epoch train loss 1.0531346036470848\n",
      "220-th epoch val loss 1.1243335229062403\n",
      "221-th epoch train loss 1.0508750108423237\n",
      "221-th epoch val loss 1.1215563176168581\n",
      "222-th epoch train loss 1.0487016152980004\n",
      "222-th epoch val loss 1.1188727620774455\n",
      "223-th epoch train loss 1.0466114818723662\n",
      "223-th epoch val loss 1.1162798252866413\n",
      "224-th epoch train loss 1.0446017708344795\n",
      "224-th epoch val loss 1.113774572497564\n",
      "225-th epoch train loss 1.0426697348280065\n",
      "225-th epoch val loss 1.1113541621890173\n",
      "226-th epoch train loss 1.0408127159306606\n",
      "226-th epoch val loss 1.1090158431315762\n",
      "227-th epoch train loss 1.039028142806276\n",
      "227-th epoch val loss 1.1067569515455797\n",
      "228-th epoch train loss 1.0373135279466184\n",
      "228-th epoch val loss 1.10457490834816\n",
      "229-th epoch train loss 1.0356664650001168\n",
      "229-th epoch val loss 1.1024672164865272\n",
      "230-th epoch train loss 1.0340846261847907\n",
      "230-th epoch val loss 1.1004314583548096\n",
      "231-th epoch train loss 1.0325657597827391\n",
      "231-th epoch val loss 1.0984652932918415\n",
      "232-th epoch train loss 1.0311076877136243\n",
      "232-th epoch val loss 1.0965664551573664\n",
      "233-th epoch train loss 1.0297083031846812\n",
      "233-th epoch val loss 1.0947327499842026\n",
      "234-th epoch train loss 1.0283655684148523\n",
      "234-th epoch val loss 1.0929620537040017\n",
      "235-th epoch train loss 1.0270775124307185\n",
      "235-th epoch val loss 1.0912523099442926\n",
      "236-th epoch train loss 1.0258422289319782\n",
      "236-th epoch val loss 1.0896015278945876\n",
      "237-th epoch train loss 1.0246578742242909\n",
      "237-th epoch val loss 1.0880077802393888\n",
      "238-th epoch train loss 1.0235226652173748\n",
      "238-th epoch val loss 1.086469201156003\n",
      "239-th epoch train loss 1.0224348774863086\n",
      "239-th epoch val loss 1.08498398437514\n",
      "240-th epoch train loss 1.0213928433940527\n",
      "240-th epoch val loss 1.083550381302329\n",
      "241-th epoch train loss 1.0203949502732763\n",
      "241-th epoch val loss 1.0821666991982546\n",
      "242-th epoch train loss 1.019439638665619\n",
      "242-th epoch val loss 1.080831299416165\n",
      "243-th epoch train loss 1.018525400616593\n",
      "243-th epoch val loss 1.0795425956945732\n",
      "244-th epoch train loss 1.0176507780243726\n",
      "244-th epoch val loss 1.0782990525035157\n",
      "245-th epoch train loss 1.0168143610407865\n",
      "245-th epoch val loss 1.0770991834426988\n",
      "246-th epoch train loss 1.0160147865228655\n",
      "246-th epoch val loss 1.075941549689908\n",
      "247-th epoch train loss 1.0152507365333654\n",
      "247-th epoch val loss 1.0748247584981072\n",
      "248-th epoch train loss 1.0145209368887256\n",
      "248-th epoch val loss 1.0737474617397018\n",
      "249-th epoch train loss 1.0138241557529666\n",
      "249-th epoch val loss 1.0727083544965006\n",
      "250-th epoch train loss 1.0131592022760958\n",
      "250-th epoch val loss 1.0717061736939328\n",
      "251-th epoch train loss 1.0125249252756146\n",
      "251-th epoch val loss 1.070739696778149\n",
      "252-th epoch train loss 1.0119202119597719\n",
      "252-th epoch val loss 1.069807740434651\n",
      "253-th epoch train loss 1.0113439866912586\n",
      "253-th epoch val loss 1.0689091593471636\n",
      "254-th epoch train loss 1.0107952097900679\n",
      "254-th epoch val loss 1.0680428449954809\n",
      "255-th epoch train loss 1.0102728763742923\n",
      "255-th epoch val loss 1.0672077244910714\n",
      "256-th epoch train loss 1.00977601523766\n",
      "256-th epoch val loss 1.0664027594492564\n",
      "257-th epoch train loss 1.0093036877626667\n",
      "257-th epoch val loss 1.0656269448968234\n",
      "258-th epoch train loss 1.0088549868681689\n",
      "258-th epoch val loss 1.0648793082139585\n",
      "259-th epoch train loss 1.0084290359903674\n",
      "259-th epoch val loss 1.064158908109426\n",
      "260-th epoch train loss 1.0080249880961243\n",
      "260-th epoch val loss 1.063464833627959\n",
      "261-th epoch train loss 1.0076420247276012\n",
      "261-th epoch val loss 1.0627962031888458\n",
      "262-th epoch train loss 1.0072793550772245\n",
      "262-th epoch val loss 1.0621521636547377\n",
      "263-th epoch train loss 1.0069362150920353\n",
      "263-th epoch val loss 1.0615318894297363\n",
      "264-th epoch train loss 1.0066118666064912\n",
      "264-th epoch val loss 1.060934581585835\n",
      "265-th epoch train loss 1.006305596502826\n",
      "265-th epoch val loss 1.0603594670168386\n",
      "266-th epoch train loss 1.0060167158981002\n",
      "266-th epoch val loss 1.0598057976188844\n",
      "267-th epoch train loss 1.005744559357107\n",
      "267-th epoch val loss 1.059272849496752\n",
      "268-th epoch train loss 1.0054884841303084\n",
      "268-th epoch val loss 1.058759922195134\n",
      "269-th epoch train loss 1.0052478694160238\n",
      "269-th epoch val loss 1.0582663379540997\n",
      "270-th epoch train loss 1.0050221156461043\n",
      "270-th epoch val loss 1.0577914409879878\n",
      "271-th epoch train loss 1.0048106437943485\n",
      "271-th epoch val loss 1.057334596786998\n",
      "272-th epoch train loss 1.0046128947069473\n",
      "272-th epoch val loss 1.0568951914407618\n",
      "273-th epoch train loss 1.004428328454267\n",
      "273-th epoch val loss 1.0564726309832198\n",
      "274-th epoch train loss 1.004256423703288\n",
      "274-th epoch val loss 1.0560663407581186\n",
      "275-th epoch train loss 1.0040966771100577\n",
      "275-th epoch val loss 1.0556757648044957\n",
      "276-th epoch train loss 1.003948602731522\n",
      "276-th epoch val loss 1.055300365261523\n",
      "277-th epoch train loss 1.003811731456124\n",
      "277-th epoch val loss 1.0549396217920946\n",
      "278-th epoch train loss 1.0036856104525813\n",
      "278-th epoch val loss 1.0545930310245817\n",
      "279-th epoch train loss 1.0035698026362638\n",
      "279-th epoch val loss 1.0542601060121781\n",
      "280-th epoch train loss 1.0034638861526208\n",
      "280-th epoch val loss 1.05394037570929\n",
      "281-th epoch train loss 1.0033674538771171\n",
      "281-th epoch val loss 1.0536333844644297\n",
      "282-th epoch train loss 1.0032801129311564\n",
      "282-th epoch val loss 1.0533386915291\n",
      "283-th epoch train loss 1.0032014842134904\n",
      "283-th epoch val loss 1.0530558705821698\n",
      "284-th epoch train loss 1.0031312019466223\n",
      "284-th epoch val loss 1.0527845092692494\n",
      "285-th epoch train loss 1.003068913237729\n",
      "285-th epoch val loss 1.0525242087565987\n",
      "286-th epoch train loss 1.00301427765365\n",
      "286-th epoch val loss 1.0522745832991116\n",
      "287-th epoch train loss 1.0029669668094878\n",
      "287-th epoch val loss 1.0520352598219351\n",
      "288-th epoch train loss 1.0029266639704053\n",
      "288-th epoch val loss 1.051805877515297\n",
      "289-th epoch train loss 1.002893063666184\n",
      "289-th epoch val loss 1.0515860874421237\n",
      "290-th epoch train loss 1.0028658713181593\n",
      "290-th epoch val loss 1.0513755521580535\n",
      "291-th epoch train loss 1.0028448028781212\n",
      "291-th epoch val loss 1.051173945343446\n",
      "292-th epoch train loss 1.0028295844788206\n",
      "292-th epoch val loss 1.050980951447024\n",
      "293-th epoch train loss 1.0028199520957028\n",
      "293-th epoch val loss 1.0507962653407714\n",
      "294-th epoch train loss 1.0028156512195145\n",
      "294-th epoch val loss 1.0506195919857417\n",
      "295-th epoch train loss 1.0028164365394434\n",
      "295-th epoch val loss 1.0504506461084302\n",
      "296-th epoch train loss 1.0028220716364529\n",
      "296-th epoch val loss 1.050289151887377\n",
      "297-th epoch train loss 1.0028323286864933\n",
      "297-th epoch val loss 1.0501348426496895\n",
      "298-th epoch train loss 1.0028469881732738\n",
      "298-th epoch val loss 1.0499874605771558\n",
      "299-th epoch train loss 1.0028658386102909\n",
      "299-th epoch val loss 1.0498467564216647\n",
      "300-th epoch train loss 1.0028886762718239\n",
      "300-th epoch val loss 1.0497124892296303\n",
      "301-th epoch train loss 1.0029153049326112\n",
      "301-th epoch val loss 1.0495844260751388\n",
      "302-th epoch train loss 1.0029455356159303\n",
      "302-th epoch val loss 1.0494623418015503\n",
      "303-th epoch train loss 1.002979186349818\n",
      "303-th epoch val loss 1.0493460187712782\n",
      "304-th epoch train loss 1.0030160819311735\n",
      "304-th epoch val loss 1.0492352466235024\n",
      "305-th epoch train loss 1.0030560536974844\n",
      "305-th epoch val loss 1.049129822039557\n",
      "306-th epoch train loss 1.0030989393059504\n",
      "306-th epoch val loss 1.0490295485157572\n",
      "307-th epoch train loss 1.003144582519755\n",
      "307-th epoch val loss 1.0489342361434237\n",
      "308-th epoch train loss 1.0031928330012643\n",
      "308-th epoch val loss 1.0488437013958936\n",
      "309-th epoch train loss 1.0032435461119318\n",
      "309-th epoch val loss 1.0487577669222772\n",
      "310-th epoch train loss 1.0032965827186993\n",
      "310-th epoch val loss 1.0486762613477674\n",
      "311-th epoch train loss 1.003351809006681\n",
      "311-th epoch val loss 1.0485990190802847\n",
      "312-th epoch train loss 1.0034090962979347\n",
      "312-th epoch val loss 1.0485258801232626\n",
      "313-th epoch train loss 1.003468320876128\n",
      "313-th epoch val loss 1.0484566898943766\n",
      "314-th epoch train loss 1.0035293638169083\n",
      "314-th epoch val loss 1.048391299050037\n",
      "315-th epoch train loss 1.003592110823797\n",
      "315-th epoch val loss 1.0483295633154543\n",
      "316-th epoch train loss 1.003656452069432\n",
      "316-th epoch val loss 1.0482713433201103\n",
      "317-th epoch train loss 1.0037222820419878\n",
      "317-th epoch val loss 1.0482165044384597\n",
      "318-th epoch train loss 1.0037894993966068\n",
      "318-th epoch val loss 1.0481649166357028\n",
      "319-th epoch train loss 1.0038580068116838\n",
      "319-th epoch val loss 1.0481164543184565\n",
      "320-th epoch train loss 1.0039277108498508\n",
      "320-th epoch val loss 1.0480709961901906\n",
      "321-th epoch train loss 1.003998521823506\n",
      "321-th epoch val loss 1.0480284251112608\n",
      "322-th epoch train loss 1.0040703536647475\n",
      "322-th epoch val loss 1.047988627963399\n",
      "323-th epoch train loss 1.0041431237995708\n",
      "323-th epoch val loss 1.047951495518525\n",
      "324-th epoch train loss 1.0042167530261874\n",
      "324-th epoch val loss 1.0479169223117393\n",
      "325-th epoch train loss 1.0042911653973463\n",
      "325-th epoch val loss 1.0478848065183652\n",
      "326-th epoch train loss 1.004366288106511\n",
      "326-th epoch val loss 1.0478550498349122\n",
      "327-th epoch train loss 1.0044420513777896\n",
      "327-th epoch val loss 1.0478275573638416\n",
      "328-th epoch train loss 1.004518388359481\n",
      "328-th epoch val loss 1.0478022375020057\n",
      "329-th epoch train loss 1.0045952350211327\n",
      "329-th epoch val loss 1.0477790018326543\n",
      "330-th epoch train loss 1.0046725300539892\n",
      "330-th epoch val loss 1.0477577650208902\n",
      "331-th epoch train loss 1.004750214774733\n",
      "331-th epoch val loss 1.0477384447124634\n",
      "332-th epoch train loss 1.004828233032396\n",
      "332-th epoch val loss 1.0477209614358058\n",
      "333-th epoch train loss 1.0049065311183638\n",
      "333-th epoch val loss 1.047705238507198\n",
      "334-th epoch train loss 1.0049850576793444\n",
      "334-th epoch val loss 1.0476912019389668\n",
      "335-th epoch train loss 1.005063763633235\n",
      "335-th epoch val loss 1.0476787803506298\n",
      "336-th epoch train loss 1.0051426020877703\n",
      "336-th epoch val loss 1.047667904882878\n",
      "337-th epoch train loss 1.0052215282618793\n",
      "337-th epoch val loss 1.0476585091143216\n",
      "338-th epoch train loss 1.0053004994096544\n",
      "338-th epoch val loss 1.0476505289808984\n",
      "339-th epoch train loss 1.0053794747468519\n",
      "339-th epoch val loss 1.0476439026978712\n",
      "340-th epoch train loss 1.0054584153798432\n",
      "340-th epoch val loss 1.0476385706843294\n",
      "341-th epoch train loss 1.0055372842369334\n",
      "341-th epoch val loss 1.047634475490108\n",
      "342-th epoch train loss 1.0056160460019812\n",
      "342-th epoch val loss 1.0476315617250616\n",
      "343-th epoch train loss 1.0056946670502296\n",
      "343-th epoch val loss 1.0476297759906044\n",
      "344-th epoch train loss 1.005773115386295\n",
      "344-th epoch val loss 1.0476290668134565\n",
      "345-th epoch train loss 1.0058513605842292\n",
      "345-th epoch val loss 1.0476293845815192\n",
      "346-th epoch train loss 1.0059293737295967\n",
      "346-th epoch val loss 1.0476306814818146\n",
      "347-th epoch train loss 1.0060071273634994\n",
      "347-th epoch val loss 1.0476329114404204\n",
      "348-th epoch train loss 1.0060845954284836\n",
      "348-th epoch val loss 1.0476360300643461\n",
      "349-th epoch train loss 1.0061617532162725\n",
      "349-th epoch val loss 1.0476399945852777\n",
      "350-th epoch train loss 1.0062385773172644\n",
      "350-th epoch val loss 1.0476447638051412\n",
      "351-th epoch train loss 1.006315045571733\n",
      "351-th epoch val loss 1.0476502980434168\n",
      "352-th epoch train loss 1.0063911370226863\n",
      "352-th epoch val loss 1.0476565590861613\n",
      "353-th epoch train loss 1.00646683187032\n",
      "353-th epoch val loss 1.047663510136673\n",
      "354-th epoch train loss 1.0065421114280173\n",
      "354-th epoch val loss 1.0476711157677514\n",
      "355-th epoch train loss 1.0066169580798467\n",
      "355-th epoch val loss 1.0476793418755077\n",
      "356-th epoch train loss 1.0066913552395074\n",
      "356-th epoch val loss 1.04768815563466\n",
      "357-th epoch train loss 1.0067652873106756\n",
      "357-th epoch val loss 1.047697525455287\n",
      "358-th epoch train loss 1.006838739648703\n",
      "358-th epoch val loss 1.0477074209409736\n",
      "359-th epoch train loss 1.0069116985236306\n",
      "359-th epoch val loss 1.0477178128483233\n",
      "360-th epoch train loss 1.0069841510844688\n",
      "360-th epoch val loss 1.047728673047778\n",
      "361-th epoch train loss 1.0070560853247028\n",
      "361-th epoch val loss 1.0477399744857103\n",
      "362-th epoch train loss 1.0071274900489882\n",
      "362-th epoch val loss 1.0477516911477542\n",
      "363-th epoch train loss 1.007198354840993\n",
      "363-th epoch val loss 1.0477637980233199\n",
      "364-th epoch train loss 1.0072686700323483\n",
      "364-th epoch val loss 1.0477762710712673\n",
      "365-th epoch train loss 1.007338426672678\n",
      "365-th epoch val loss 1.0477890871867017\n",
      "366-th epoch train loss 1.0074076165006638\n",
      "366-th epoch val loss 1.0478022241688434\n",
      "367-th epoch train loss 1.0074762319161163\n",
      "367-th epoch val loss 1.0478156606899494\n",
      "368-th epoch train loss 1.0075442659530174\n",
      "368-th epoch val loss 1.047829376265251\n",
      "369-th epoch train loss 1.0076117122535058\n",
      "369-th epoch val loss 1.0478433512238727\n",
      "370-th epoch train loss 1.0076785650427686\n",
      "370-th epoch val loss 1.047857566680699\n",
      "371-th epoch train loss 1.0077448191048142\n",
      "371-th epoch val loss 1.0478720045091645\n",
      "372-th epoch train loss 1.0078104697590933\n",
      "372-th epoch val loss 1.047886647314933\n",
      "373-th epoch train loss 1.007875512837947\n",
      "373-th epoch val loss 1.0479014784104446\n",
      "374-th epoch train loss 1.007939944664847\n",
      "374-th epoch val loss 1.0479164817902897\n",
      "375-th epoch train loss 1.0080037620334028\n",
      "375-th epoch val loss 1.047931642107397\n",
      "376-th epoch train loss 1.0080669621871172\n",
      "376-th epoch val loss 1.0479469446500016\n",
      "377-th epoch train loss 1.0081295427998598\n",
      "377-th epoch val loss 1.047962375319373\n",
      "378-th epoch train loss 1.0081915019570327\n",
      "378-th epoch val loss 1.0479779206082729\n",
      "379-th epoch train loss 1.0082528381374123\n",
      "379-th epoch val loss 1.0479935675801269\n",
      "380-th epoch train loss 1.008313550195639\n",
      "380-th epoch val loss 1.0480093038488787\n",
      "381-th epoch train loss 1.008373637345336\n",
      "381-th epoch val loss 1.0480251175595119\n",
      "382-th epoch train loss 1.008433099142834\n",
      "382-th epoch val loss 1.0480409973692177\n",
      "383-th epoch train loss 1.0084919354714879\n",
      "383-th epoch val loss 1.0480569324291804\n",
      "384-th epoch train loss 1.008550146526554\n",
      "384-th epoch val loss 1.04807291236697\n",
      "385-th epoch train loss 1.0086077328006229\n",
      "385-th epoch val loss 1.0480889272695173\n",
      "386-th epoch train loss 1.008664695069577\n",
      "386-th epoch val loss 1.0481049676666538\n",
      "387-th epoch train loss 1.008721034379067\n",
      "387-th epoch val loss 1.0481210245151986\n",
      "388-th epoch train loss 1.008776752031477\n",
      "388-th epoch val loss 1.0481370891835742\n",
      "389-th epoch train loss 1.0088318495733755\n",
      "389-th epoch val loss 1.0481531534369373\n",
      "390-th epoch train loss 1.0088863287834224\n",
      "390-th epoch val loss 1.0481692094227986\n",
      "391-th epoch train loss 1.0089401916607295\n",
      "391-th epoch val loss 1.0481852496571296\n",
      "392-th epoch train loss 1.008993440413648\n",
      "392-th epoch val loss 1.0482012670109313\n",
      "393-th epoch train loss 1.0090460774489796\n",
      "393-th epoch val loss 1.0482172546972504\n",
      "394-th epoch train loss 1.0090981053615837\n",
      "394-th epoch val loss 1.0482332062586315\n",
      "395-th epoch train loss 1.0091495269243818\n",
      "395-th epoch val loss 1.0482491155549911\n",
      "396-th epoch train loss 1.0092003450787337\n",
      "396-th epoch val loss 1.0482649767518994\n",
      "397-th epoch train loss 1.0092505629251802\n",
      "397-th epoch val loss 1.0482807843092512\n",
      "398-th epoch train loss 1.009300183714537\n",
      "398-th epoch val loss 1.0482965329703264\n",
      "399-th epoch train loss 1.0093492108393267\n",
      "399-th epoch val loss 1.0483122177512116\n",
      "400-th epoch train loss 1.0093976478255415\n",
      "400-th epoch val loss 1.0483278339305833\n",
      "401-th epoch train loss 1.0094454983247223\n",
      "401-th epoch val loss 1.0483433770398363\n",
      "402-th epoch train loss 1.0094927661063389\n",
      "402-th epoch val loss 1.0483588428535433\n",
      "403-th epoch train loss 1.0095394550504755\n",
      "403-th epoch val loss 1.0483742273802434\n",
      "404-th epoch train loss 1.0095855691407938\n",
      "404-th epoch val loss 1.0483895268535384\n",
      "405-th epoch train loss 1.0096311124577733\n",
      "405-th epoch val loss 1.0484047377234926\n",
      "406-th epoch train loss 1.0096760891722196\n",
      "406-th epoch val loss 1.0484198566483305\n",
      "407-th epoch train loss 1.0097205035390302\n",
      "407-th epoch val loss 1.048434880486412\n",
      "408-th epoch train loss 1.0097643598912045\n",
      "408-th epoch val loss 1.0484498062884844\n",
      "409-th epoch train loss 1.009807662634098\n",
      "409-th epoch val loss 1.0484646312901988\n",
      "410-th epoch train loss 1.0098504162399056\n",
      "410-th epoch val loss 1.048479352904887\n",
      "411-th epoch train loss 1.0098926252423677\n",
      "411-th epoch val loss 1.0484939687165789\n",
      "412-th epoch train loss 1.009934294231693\n",
      "412-th epoch val loss 1.0485084764732682\n",
      "413-th epoch train loss 1.0099754278496882\n",
      "413-th epoch val loss 1.0485228740804065\n",
      "414-th epoch train loss 1.0100160307850887\n",
      "414-th epoch val loss 1.0485371595946198\n",
      "415-th epoch train loss 1.0100561077690857\n",
      "415-th epoch val loss 1.0485513312176495\n",
      "416-th epoch train loss 1.0100956635710332\n",
      "416-th epoch val loss 1.0485653872904954\n",
      "417-th epoch train loss 1.0101347029943442\n",
      "417-th epoch val loss 1.0485793262877685\n",
      "418-th epoch train loss 1.0101732308725535\n",
      "418-th epoch val loss 1.048593146812237\n",
      "419-th epoch train loss 1.0102112520655526\n",
      "419-th epoch val loss 1.048606847589564\n",
      "420-th epoch train loss 1.0102487714559858\n",
      "420-th epoch val loss 1.0486204274632283\n",
      "421-th epoch train loss 1.0102857939457999\n",
      "421-th epoch val loss 1.0486338853896247\n",
      "422-th epoch train loss 1.0103223244529476\n",
      "422-th epoch val loss 1.0486472204333332\n",
      "423-th epoch train loss 1.0103583679082346\n",
      "423-th epoch val loss 1.0486604317625594\n",
      "424-th epoch train loss 1.0103939292523065\n",
      "424-th epoch val loss 1.0486735186447314\n",
      "425-th epoch train loss 1.0104290134327734\n",
      "425-th epoch val loss 1.048686480442255\n",
      "426-th epoch train loss 1.010463625401458\n",
      "426-th epoch val loss 1.0486993166084184\n",
      "427-th epoch train loss 1.0104977701117779\n",
      "427-th epoch val loss 1.048712026683441\n",
      "428-th epoch train loss 1.0105314525162428\n",
      "428-th epoch val loss 1.048724610290669\n",
      "429-th epoch train loss 1.0105646775640675\n",
      "429-th epoch val loss 1.0487370671328953\n",
      "430-th epoch train loss 1.0105974501989037\n",
      "430-th epoch val loss 1.0487493969888246\n",
      "431-th epoch train loss 1.010629775356674\n",
      "431-th epoch val loss 1.0487615997096569\n",
      "432-th epoch train loss 1.0106616579635097\n",
      "432-th epoch val loss 1.0487736752157943\n",
      "433-th epoch train loss 1.0106931029337944\n",
      "433-th epoch val loss 1.0487856234936708\n",
      "434-th epoch train loss 1.0107241151683002\n",
      "434-th epoch val loss 1.048797444592694\n",
      "435-th epoch train loss 1.0107546995524161\n",
      "435-th epoch val loss 1.0488091386222982\n",
      "436-th epoch train loss 1.0107848609544714\n",
      "436-th epoch val loss 1.0488207057491055\n",
      "437-th epoch train loss 1.0108146042241382\n",
      "437-th epoch val loss 1.0488321461941894\n",
      "438-th epoch train loss 1.0108439341909232\n",
      "438-th epoch val loss 1.048843460230441\n",
      "439-th epoch train loss 1.0108728556627358\n",
      "439-th epoch val loss 1.0488546481800296\n",
      "440-th epoch train loss 1.0109013734245356\n",
      "440-th epoch val loss 1.0488657104119576\n",
      "441-th epoch train loss 1.010929492237051\n",
      "441-th epoch val loss 1.0488766473397082\n",
      "442-th epoch train loss 1.0109572168355727\n",
      "442-th epoch val loss 1.0488874594189757\n",
      "443-th epoch train loss 1.0109845519288119\n",
      "443-th epoch val loss 1.0488981471454857\n",
      "444-th epoch train loss 1.0110115021978283\n",
      "444-th epoch val loss 1.0489087110528938\n",
      "445-th epoch train loss 1.0110380722950185\n",
      "445-th epoch val loss 1.0489191517107637\n",
      "446-th epoch train loss 1.011064266843166\n",
      "446-th epoch val loss 1.0489294697226204\n",
      "447-th epoch train loss 1.011090090434552\n",
      "447-th epoch val loss 1.0489396657240815\n",
      "448-th epoch train loss 1.0111155476301177\n",
      "448-th epoch val loss 1.0489497403810548\n",
      "449-th epoch train loss 1.0111406429586869\n",
      "449-th epoch val loss 1.0489596943880046\n",
      "450-th epoch train loss 1.0111653809162353\n",
      "450-th epoch val loss 1.0489695284662892\n",
      "451-th epoch train loss 1.011189765965212\n",
      "451-th epoch val loss 1.0489792433625567\n",
      "452-th epoch train loss 1.0112138025339106\n",
      "452-th epoch val loss 1.0489888398472094\n",
      "453-th epoch train loss 1.01123749501588\n",
      "453-th epoch val loss 1.0489983187129175\n",
      "454-th epoch train loss 1.0112608477693885\n",
      "454-th epoch val loss 1.0490076807732034\n",
      "455-th epoch train loss 1.0112838651169231\n",
      "455-th epoch val loss 1.049016926861074\n",
      "456-th epoch train loss 1.0113065513447324\n",
      "456-th epoch val loss 1.049026057827705\n",
      "457-th epoch train loss 1.0113289107024075\n",
      "457-th epoch val loss 1.0490350745411847\n",
      "458-th epoch train loss 1.0113509474025013\n",
      "458-th epoch val loss 1.0490439778853016\n",
      "459-th epoch train loss 1.0113726656201838\n",
      "459-th epoch val loss 1.0490527687583824\n",
      "460-th epoch train loss 1.0113940694929298\n",
      "460-th epoch val loss 1.0490614480721803\n",
      "461-th epoch train loss 1.0114151631202402\n",
      "461-th epoch val loss 1.0490700167507998\n",
      "462-th epoch train loss 1.0114359505633974\n",
      "462-th epoch val loss 1.049078475729676\n",
      "463-th epoch train loss 1.011456435845248\n",
      "463-th epoch val loss 1.0490868259545867\n",
      "464-th epoch train loss 1.0114766229500147\n",
      "464-th epoch val loss 1.0490950683807119\n",
      "465-th epoch train loss 1.011496515823137\n",
      "465-th epoch val loss 1.0491032039717263\n",
      "466-th epoch train loss 1.011516118371137\n",
      "466-th epoch val loss 1.0491112336989348\n",
      "467-th epoch train loss 1.0115354344615113\n",
      "467-th epoch val loss 1.0491191585404425\n",
      "468-th epoch train loss 1.0115544679226482\n",
      "468-th epoch val loss 1.0491269794803604\n",
      "469-th epoch train loss 1.0115732225437646\n",
      "469-th epoch val loss 1.0491346975080418\n",
      "470-th epoch train loss 1.0115917020748693\n",
      "470-th epoch val loss 1.0491423136173565\n",
      "471-th epoch train loss 1.0116099102267428\n",
      "471-th epoch val loss 1.049149828805995\n",
      "472-th epoch train loss 1.0116278506709404\n",
      "472-th epoch val loss 1.0491572440747978\n",
      "473-th epoch train loss 1.011645527039813\n",
      "473-th epoch val loss 1.0491645604271256\n",
      "474-th epoch train loss 1.0116629429265476\n",
      "474-th epoch val loss 1.0491717788682442\n",
      "475-th epoch train loss 1.0116801018852233\n",
      "475-th epoch val loss 1.0491789004047483\n",
      "476-th epoch train loss 1.011697007430884\n",
      "476-th epoch val loss 1.0491859260440026\n",
      "477-th epoch train loss 1.0117136630396282\n",
      "477-th epoch val loss 1.0491928567936157\n",
      "478-th epoch train loss 1.0117300721487141\n",
      "478-th epoch val loss 1.0491996936609351\n",
      "479-th epoch train loss 1.0117462381566746\n",
      "479-th epoch val loss 1.0492064376525614\n",
      "480-th epoch train loss 1.011762164423453\n",
      "480-th epoch val loss 1.049213089773894\n",
      "481-th epoch train loss 1.0117778542705433\n",
      "481-th epoch val loss 1.0492196510286935\n",
      "482-th epoch train loss 1.0117933109811512\n",
      "482-th epoch val loss 1.0492261224186648\n",
      "483-th epoch train loss 1.011808537800358\n",
      "483-th epoch val loss 1.0492325049430622\n",
      "484-th epoch train loss 1.0118235379353036\n",
      "484-th epoch val loss 1.0492387995983121\n",
      "485-th epoch train loss 1.0118383145553713\n",
      "485-th epoch val loss 1.0492450073776591\n",
      "486-th epoch train loss 1.0118528707923917\n",
      "486-th epoch val loss 1.0492511292708244\n",
      "487-th epoch train loss 1.0118672097408477\n",
      "487-th epoch val loss 1.0492571662636851\n",
      "488-th epoch train loss 1.0118813344580904\n",
      "488-th epoch val loss 1.0492631193379685\n",
      "489-th epoch train loss 1.0118952479645682\n",
      "489-th epoch val loss 1.0492689894709655\n",
      "490-th epoch train loss 1.0119089532440557\n",
      "490-th epoch val loss 1.0492747776352571\n",
      "491-th epoch train loss 1.0119224532438937\n",
      "491-th epoch val loss 1.049280484798454\n",
      "492-th epoch train loss 1.0119357508752387\n",
      "492-th epoch val loss 1.0492861119229577\n",
      "493-th epoch train loss 1.011948849013312\n",
      "493-th epoch val loss 1.0492916599657256\n",
      "494-th epoch train loss 1.0119617504976604\n",
      "494-th epoch val loss 1.0492971298780582\n",
      "495-th epoch train loss 1.0119744581324202\n",
      "495-th epoch val loss 1.0493025226053945\n",
      "496-th epoch train loss 1.0119869746865853\n",
      "496-th epoch val loss 1.049307839087119\n",
      "497-th epoch train loss 1.011999302894283\n",
      "497-th epoch val loss 1.0493130802563866\n",
      "498-th epoch train loss 1.0120114454550513\n",
      "498-th epoch val loss 1.0493182470399498\n",
      "499-th epoch train loss 1.0120234050341195\n",
      "499-th epoch val loss 1.0493233403580047\n",
      "500-th epoch train loss 1.0120351842626987\n",
      "500-th epoch val loss 1.0493283611240423\n",
      "501-th epoch train loss 1.0120467857382665\n",
      "501-th epoch val loss 1.0493333102447138\n",
      "502-th epoch train loss 1.012058212024863\n",
      "502-th epoch val loss 1.049338188619703\n",
      "503-th epoch train loss 1.0120694656533844\n",
      "503-th epoch val loss 1.049342997141609\n",
      "504-th epoch train loss 1.0120805491218803\n",
      "504-th epoch val loss 1.0493477366958364\n",
      "505-th epoch train loss 1.0120914648958568\n",
      "505-th epoch val loss 1.0493524081604972\n",
      "506-th epoch train loss 1.0121022154085757\n",
      "506-th epoch val loss 1.049357012406318\n",
      "507-th epoch train loss 1.012112803061359\n",
      "507-th epoch val loss 1.0493615502965548\n",
      "508-th epoch train loss 1.0121232302238963\n",
      "508-th epoch val loss 1.04936602268692\n",
      "509-th epoch train loss 1.0121334992345494\n",
      "509-th epoch val loss 1.0493704304255091\n",
      "510-th epoch train loss 1.0121436124006613\n",
      "510-th epoch val loss 1.0493747743527417\n",
      "511-th epoch train loss 1.012153571998866\n",
      "511-th epoch val loss 1.0493790553013054\n",
      "512-th epoch train loss 1.0121633802753949\n",
      "512-th epoch val loss 1.0493832740961047\n",
      "513-th epoch train loss 1.0121730394463901\n",
      "513-th epoch val loss 1.0493874315542198\n",
      "514-th epoch train loss 1.0121825516982148\n",
      "514-th epoch val loss 1.0493915284848683\n",
      "515-th epoch train loss 1.0121919191877615\n",
      "515-th epoch val loss 1.049395565689376\n",
      "516-th epoch train loss 1.0122011440427638\n",
      "516-th epoch val loss 1.0493995439611459\n",
      "517-th epoch train loss 1.0122102283621084\n",
      "517-th epoch val loss 1.049403464085641\n",
      "518-th epoch train loss 1.012219174216143\n",
      "518-th epoch val loss 1.0494073268403659\n",
      "519-th epoch train loss 1.0122279836469883\n",
      "519-th epoch val loss 1.0494111329948577\n",
      "520-th epoch train loss 1.0122366586688454\n",
      "520-th epoch val loss 1.049414883310674\n",
      "521-th epoch train loss 1.0122452012683054\n",
      "521-th epoch val loss 1.0494185785413954\n",
      "522-th epoch train loss 1.0122536134046578\n",
      "522-th epoch val loss 1.0494222194326213\n",
      "523-th epoch train loss 1.0122618970101969\n",
      "523-th epoch val loss 1.0494258067219795\n",
      "524-th epoch train loss 1.0122700539905265\n",
      "524-th epoch val loss 1.0494293411391318\n",
      "525-th epoch train loss 1.0122780862248664\n",
      "525-th epoch val loss 1.0494328234057873\n",
      "526-th epoch train loss 1.0122859955663555\n",
      "526-th epoch val loss 1.049436254235716\n",
      "527-th epoch train loss 1.0122937838423536\n",
      "527-th epoch val loss 1.0494396343347718\n",
      "528-th epoch train loss 1.0123014528547427\n",
      "528-th epoch val loss 1.049442964400908\n",
      "529-th epoch train loss 1.0123090043802268\n",
      "529-th epoch val loss 1.0494462451242077\n",
      "530-th epoch train loss 1.0123164401706282\n",
      "530-th epoch val loss 1.0494494771869063\n",
      "531-th epoch train loss 1.012323761953186\n",
      "531-th epoch val loss 1.0494526612634243\n",
      "532-th epoch train loss 1.012330971430848\n",
      "532-th epoch val loss 1.0494557980203982\n",
      "533-th epoch train loss 1.0123380702825662\n",
      "533-th epoch val loss 1.0494588881167157\n",
      "534-th epoch train loss 1.0123450601635853\n",
      "534-th epoch val loss 1.0494619322035517\n",
      "535-th epoch train loss 1.012351942705733\n",
      "535-th epoch val loss 1.049464930924407\n",
      "536-th epoch train loss 1.0123587195177075\n",
      "536-th epoch val loss 1.0494678849151513\n",
      "537-th epoch train loss 1.0123653921853606\n",
      "537-th epoch val loss 1.0494707948040627\n",
      "538-th epoch train loss 1.0123719622719838\n",
      "538-th epoch val loss 1.0494736612118742\n",
      "539-th epoch train loss 1.0123784313185864\n",
      "539-th epoch val loss 1.049476484751818\n",
      "540-th epoch train loss 1.0123848008441756\n",
      "540-th epoch val loss 1.0494792660296757\n",
      "541-th epoch train loss 1.0123910723460345\n",
      "541-th epoch val loss 1.0494820056438243\n",
      "542-th epoch train loss 1.0123972472999938\n",
      "542-th epoch val loss 1.0494847041852884\n",
      "543-th epoch train loss 1.0124033271607062\n",
      "543-th epoch val loss 1.0494873622377914\n",
      "544-th epoch train loss 1.012409313361917\n",
      "544-th epoch val loss 1.0494899803778084\n",
      "545-th epoch train loss 1.012415207316728\n",
      "545-th epoch val loss 1.049492559174619\n",
      "546-th epoch train loss 1.0124210104178684\n",
      "546-th epoch val loss 1.0494950991903642\n",
      "547-th epoch train loss 1.0124267240379528\n",
      "547-th epoch val loss 1.0494976009801007\n",
      "548-th epoch train loss 1.012432349529746\n",
      "548-th epoch val loss 1.0495000650918582\n",
      "549-th epoch train loss 1.0124378882264191\n",
      "549-th epoch val loss 1.0495024920666964\n",
      "550-th epoch train loss 1.0124433414418053\n",
      "550-th epoch val loss 1.0495048824387652\n",
      "551-th epoch train loss 1.0124487104706548\n",
      "551-th epoch val loss 1.0495072367353606\n",
      "552-th epoch train loss 1.0124539965888857\n",
      "552-th epoch val loss 1.049509555476987\n",
      "553-th epoch train loss 1.0124592010538314\n",
      "553-th epoch val loss 1.0495118391774167\n",
      "554-th epoch train loss 1.0124643251044885\n",
      "554-th epoch val loss 1.049514088343749\n",
      "555-th epoch train loss 1.0124693699617606\n",
      "555-th epoch val loss 1.0495163034764743\n",
      "556-th epoch train loss 1.0124743368286975\n",
      "556-th epoch val loss 1.0495184850695316\n",
      "557-th epoch train loss 1.0124792268907377\n",
      "557-th epoch val loss 1.0495206336103748\n",
      "558-th epoch train loss 1.0124840413159413\n",
      "558-th epoch val loss 1.0495227495800323\n",
      "559-th epoch train loss 1.012488781255227\n",
      "559-th epoch val loss 1.0495248334531688\n",
      "560-th epoch train loss 1.0124934478426015\n",
      "560-th epoch val loss 1.0495268856981506\n",
      "561-th epoch train loss 1.012498042195389\n",
      "561-th epoch val loss 1.0495289067771052\n",
      "562-th epoch train loss 1.0125025654144604\n",
      "562-th epoch val loss 1.049530897145988\n",
      "563-th epoch train loss 1.0125070185844534\n",
      "563-th epoch val loss 1.0495328572546425\n",
      "564-th epoch train loss 1.0125114027739968\n",
      "564-th epoch val loss 1.0495347875468644\n",
      "565-th epoch train loss 1.01251571903593\n",
      "565-th epoch val loss 1.0495366884604644\n",
      "566-th epoch train loss 1.0125199684075177\n",
      "566-th epoch val loss 1.0495385604273322\n",
      "567-th epoch train loss 1.0125241519106682\n",
      "567-th epoch val loss 1.0495404038735\n",
      "568-th epoch train loss 1.0125282705521395\n",
      "568-th epoch val loss 1.0495422192192032\n",
      "569-th epoch train loss 1.0125323253237557\n",
      "569-th epoch val loss 1.0495440068789466\n",
      "570-th epoch train loss 1.0125363172026087\n",
      "570-th epoch val loss 1.0495457672615647\n",
      "571-th epoch train loss 1.0125402471512668\n",
      "571-th epoch val loss 1.0495475007702866\n",
      "572-th epoch train loss 1.0125441161179725\n",
      "572-th epoch val loss 1.049549207802796\n",
      "573-th epoch train loss 1.0125479250368496\n",
      "573-th epoch val loss 1.0495508887512963\n",
      "574-th epoch train loss 1.0125516748280932\n",
      "574-th epoch val loss 1.0495525440025713\n",
      "575-th epoch train loss 1.0125553663981703\n",
      "575-th epoch val loss 1.049554173938047\n",
      "576-th epoch train loss 1.0125590006400103\n",
      "576-th epoch val loss 1.0495557789338545\n",
      "577-th epoch train loss 1.0125625784331964\n",
      "577-th epoch val loss 1.0495573593608898\n",
      "578-th epoch train loss 1.0125661006441544\n",
      "578-th epoch val loss 1.0495589155848775\n",
      "579-th epoch train loss 1.0125695681263376\n",
      "579-th epoch val loss 1.049560447966428\n",
      "580-th epoch train loss 1.0125729817204119\n",
      "580-th epoch val loss 1.0495619568611014\n",
      "581-th epoch train loss 1.0125763422544358\n",
      "581-th epoch val loss 1.0495634426194655\n",
      "582-th epoch train loss 1.0125796505440414\n",
      "582-th epoch val loss 1.0495649055871574\n",
      "583-th epoch train loss 1.0125829073926096\n",
      "583-th epoch val loss 1.049566346104942\n",
      "584-th epoch train loss 1.0125861135914471\n",
      "584-th epoch val loss 1.0495677645087702\n",
      "585-th epoch train loss 1.0125892699199561\n",
      "585-th epoch val loss 1.0495691611298397\n",
      "586-th epoch train loss 1.0125923771458087\n",
      "586-th epoch val loss 1.0495705362946515\n",
      "587-th epoch train loss 1.0125954360251102\n",
      "587-th epoch val loss 1.0495718903250697\n",
      "588-th epoch train loss 1.0125984473025709\n",
      "588-th epoch val loss 1.0495732235383772\n",
      "589-th epoch train loss 1.0126014117116648\n",
      "589-th epoch val loss 1.049574536247333\n",
      "590-th epoch train loss 1.012604329974795\n",
      "590-th epoch val loss 1.0495758287602308\n",
      "591-th epoch train loss 1.012607202803453\n",
      "591-th epoch val loss 1.049577101380953\n",
      "592-th epoch train loss 1.0126100308983748\n",
      "592-th epoch val loss 1.0495783544090267\n",
      "593-th epoch train loss 1.0126128149496987\n",
      "593-th epoch val loss 1.0495795881396806\n",
      "594-th epoch train loss 1.0126155556371157\n",
      "594-th epoch val loss 1.049580802863898\n",
      "595-th epoch train loss 1.0126182536300274\n",
      "595-th epoch val loss 1.0495819988684731\n",
      "596-th epoch train loss 1.0126209095876884\n",
      "596-th epoch val loss 1.0495831764360628\n",
      "597-th epoch train loss 1.0126235241593586\n",
      "597-th epoch val loss 1.049584335845242\n",
      "598-th epoch train loss 1.0126260979844475\n",
      "598-th epoch val loss 1.049585477370555\n",
      "599-th epoch train loss 1.0126286316926578\n",
      "599-th epoch val loss 1.0495866012825692\n",
      "600-th epoch train loss 1.0126311259041278\n",
      "600-th epoch val loss 1.0495877078479263\n",
      "601-th epoch train loss 1.0126335812295715\n",
      "601-th epoch val loss 1.0495887973293947\n",
      "602-th epoch train loss 1.0126359982704163\n",
      "602-th epoch val loss 1.0495898699859196\n",
      "603-th epoch train loss 1.0126383776189387\n",
      "603-th epoch val loss 1.0495909260726741\n",
      "604-th epoch train loss 1.0126407198584009\n",
      "604-th epoch val loss 1.0495919658411088\n",
      "605-th epoch train loss 1.0126430255631798\n",
      "605-th epoch val loss 1.049592989539001\n",
      "606-th epoch train loss 1.0126452952989016\n",
      "606-th epoch val loss 1.0495939974105055\n",
      "607-th epoch train loss 1.0126475296225677\n",
      "607-th epoch val loss 1.0495949896962007\n",
      "608-th epoch train loss 1.0126497290826848\n",
      "608-th epoch val loss 1.0495959666331385\n",
      "609-th epoch train loss 1.012651894219387\n",
      "609-th epoch val loss 1.0495969284548903\n",
      "610-th epoch train loss 1.0126540255645635\n",
      "610-th epoch val loss 1.0495978753915964\n",
      "611-th epoch train loss 1.0126561236419767\n",
      "611-th epoch val loss 1.0495988076700093\n",
      "612-th epoch train loss 1.0126581889673854\n",
      "612-th epoch val loss 1.049599725513542\n",
      "613-th epoch train loss 1.0126602220486638\n",
      "613-th epoch val loss 1.0496006291423141\n",
      "614-th epoch train loss 1.0126622233859157\n",
      "614-th epoch val loss 1.0496015187731929\n",
      "615-th epoch train loss 1.0126641934715925\n",
      "615-th epoch val loss 1.0496023946198445\n",
      "616-th epoch train loss 1.012666132790607\n",
      "616-th epoch val loss 1.0496032568927702\n",
      "617-th epoch train loss 1.0126680418204435\n",
      "617-th epoch val loss 1.049604105799356\n",
      "618-th epoch train loss 1.0126699210312706\n",
      "618-th epoch val loss 1.0496049415439137\n",
      "619-th epoch train loss 1.0126717708860506\n",
      "619-th epoch val loss 1.0496057643277223\n",
      "620-th epoch train loss 1.012673591840644\n",
      "620-th epoch val loss 1.0496065743490715\n",
      "621-th epoch train loss 1.0126753843439202\n",
      "621-th epoch val loss 1.0496073718033034\n",
      "622-th epoch train loss 1.0126771488378576\n",
      "622-th epoch val loss 1.0496081568828515\n",
      "623-th epoch train loss 1.0126788857576503\n",
      "623-th epoch val loss 1.0496089297772837\n",
      "624-th epoch train loss 1.0126805955318074\n",
      "624-th epoch val loss 1.049609690673342\n",
      "625-th epoch train loss 1.0126822785822542\n",
      "625-th epoch val loss 1.0496104397549815\n",
      "626-th epoch train loss 1.0126839353244315\n",
      "626-th epoch val loss 1.0496111772034091\n",
      "627-th epoch train loss 1.0126855661673906\n",
      "627-th epoch val loss 1.0496119031971236\n",
      "628-th epoch train loss 1.0126871715138936\n",
      "628-th epoch val loss 1.0496126179119531\n",
      "629-th epoch train loss 1.0126887517605025\n",
      "629-th epoch val loss 1.0496133215210928\n",
      "630-th epoch train loss 1.0126903072976783\n",
      "630-th epoch val loss 1.0496140141951424\n",
      "631-th epoch train loss 1.012691838509867\n",
      "631-th epoch val loss 1.0496146961021435\n",
      "632-th epoch train loss 1.0126933457755969\n",
      "632-th epoch val loss 1.049615367407614\n",
      "633-th epoch train loss 1.0126948294675608\n",
      "633-th epoch val loss 1.0496160282745872\n",
      "634-th epoch train loss 1.0126962899527092\n",
      "634-th epoch val loss 1.0496166788636458\n",
      "635-th epoch train loss 1.0126977275923348\n",
      "635-th epoch val loss 1.049617319332954\n",
      "636-th epoch train loss 1.0126991427421597\n",
      "636-th epoch val loss 1.0496179498382985\n",
      "637-th epoch train loss 1.0127005357524164\n",
      "637-th epoch val loss 1.0496185705331165\n",
      "638-th epoch train loss 1.012701906967935\n",
      "638-th epoch val loss 1.049619181568533\n",
      "639-th epoch train loss 1.0127032567282213\n",
      "639-th epoch val loss 1.0496197830933924\n",
      "640-th epoch train loss 1.0127045853675407\n",
      "640-th epoch val loss 1.049620375254293\n",
      "641-th epoch train loss 1.0127058932149944\n",
      "641-th epoch val loss 1.049620958195617\n",
      "642-th epoch train loss 1.0127071805946006\n",
      "642-th epoch val loss 1.049621532059566\n",
      "643-th epoch train loss 1.0127084478253694\n",
      "643-th epoch val loss 1.0496220969861885\n",
      "644-th epoch train loss 1.012709695221381\n",
      "644-th epoch val loss 1.0496226531134147\n",
      "645-th epoch train loss 1.0127109230918596\n",
      "645-th epoch val loss 1.0496232005770851\n",
      "646-th epoch train loss 1.012712131741245\n",
      "646-th epoch val loss 1.049623739510981\n",
      "647-th epoch train loss 1.0127133214692698\n",
      "647-th epoch val loss 1.0496242700468557\n",
      "648-th epoch train loss 1.0127144925710287\n",
      "648-th epoch val loss 1.049624792314465\n",
      "649-th epoch train loss 1.0127156453370476\n",
      "649-th epoch val loss 1.049625306441591\n",
      "650-th epoch train loss 1.0127167800533563\n",
      "650-th epoch val loss 1.049625812554079\n",
      "651-th epoch train loss 1.0127178970015547\n",
      "651-th epoch val loss 1.0496263107758579\n",
      "652-th epoch train loss 1.012718996458881\n",
      "652-th epoch val loss 1.0496268012289744\n",
      "653-th epoch train loss 1.0127200786982795\n",
      "653-th epoch val loss 1.0496272840336167\n",
      "654-th epoch train loss 1.0127211439884647\n",
      "654-th epoch val loss 1.0496277593081447\n",
      "655-th epoch train loss 1.0127221925939858\n",
      "655-th epoch val loss 1.0496282271691146\n",
      "656-th epoch train loss 1.0127232247752906\n",
      "656-th epoch val loss 1.0496286877313066\n",
      "657-th epoch train loss 1.0127242407887902\n",
      "657-th epoch val loss 1.0496291411077507\n",
      "658-th epoch train loss 1.0127252408869167\n",
      "658-th epoch val loss 1.0496295874097525\n",
      "659-th epoch train loss 1.0127262253181881\n",
      "659-th epoch val loss 1.0496300267469196\n",
      "660-th epoch train loss 1.0127271943272664\n",
      "660-th epoch val loss 1.049630459227185\n",
      "661-th epoch train loss 1.012728148155015\n",
      "661-th epoch val loss 1.0496308849568332\n",
      "662-th epoch train loss 1.012729087038561\n",
      "662-th epoch val loss 1.049631304040525\n",
      "663-th epoch train loss 1.012730011211349\n",
      "663-th epoch val loss 1.04963171658132\n",
      "664-th epoch train loss 1.0127309209031987\n",
      "664-th epoch val loss 1.049632122680701\n",
      "665-th epoch train loss 1.0127318163403605\n",
      "665-th epoch val loss 1.0496325224385994\n",
      "666-th epoch train loss 1.01273269774557\n",
      "666-th epoch val loss 1.0496329159534143\n",
      "667-th epoch train loss 1.0127335653381027\n",
      "667-th epoch val loss 1.0496333033220389\n",
      "668-th epoch train loss 1.0127344193338268\n",
      "668-th epoch val loss 1.0496336846398817\n",
      "669-th epoch train loss 1.0127352599452544\n",
      "669-th epoch val loss 1.049634060000888\n",
      "670-th epoch train loss 1.0127360873815954\n",
      "670-th epoch val loss 1.0496344294975626\n",
      "671-th epoch train loss 1.0127369018488066\n",
      "671-th epoch val loss 1.0496347932209913\n",
      "672-th epoch train loss 1.0127377035496412\n",
      "672-th epoch val loss 1.049635151260861\n",
      "673-th epoch train loss 1.0127384926837\n",
      "673-th epoch val loss 1.0496355037054834\n",
      "674-th epoch train loss 1.0127392694474784\n",
      "674-th epoch val loss 1.0496358506418115\n",
      "675-th epoch train loss 1.0127400340344153\n",
      "675-th epoch val loss 1.049636192155464\n",
      "676-th epoch train loss 1.0127407866349383\n",
      "676-th epoch val loss 1.0496365283307423\n",
      "677-th epoch train loss 1.0127415274365128\n",
      "677-th epoch val loss 1.0496368592506515\n",
      "678-th epoch train loss 1.0127422566236852\n",
      "678-th epoch val loss 1.0496371849969215\n",
      "679-th epoch train loss 1.01274297437813\n",
      "679-th epoch val loss 1.0496375056500231\n",
      "680-th epoch train loss 1.0127436808786914\n",
      "680-th epoch val loss 1.0496378212891881\n",
      "681-th epoch train loss 1.0127443763014303\n",
      "681-th epoch val loss 1.0496381319924297\n",
      "682-th epoch train loss 1.0127450608196646\n",
      "682-th epoch val loss 1.049638437836559\n",
      "683-th epoch train loss 1.0127457346040125\n",
      "683-th epoch val loss 1.0496387388972026\n",
      "684-th epoch train loss 1.0127463978224343\n",
      "684-th epoch val loss 1.0496390352488225\n",
      "685-th epoch train loss 1.0127470506402727\n",
      "685-th epoch val loss 1.0496393269647328\n",
      "686-th epoch train loss 1.0127476932202948\n",
      "686-th epoch val loss 1.0496396141171158\n",
      "687-th epoch train loss 1.0127483257227305\n",
      "687-th epoch val loss 1.0496398967770415\n",
      "688-th epoch train loss 1.0127489483053114\n",
      "688-th epoch val loss 1.0496401750144824\n",
      "689-th epoch train loss 1.0127495611233113\n",
      "689-th epoch val loss 1.0496404488983309\n",
      "690-th epoch train loss 1.0127501643295826\n",
      "690-th epoch val loss 1.049640718496416\n",
      "691-th epoch train loss 1.0127507580745938\n",
      "691-th epoch val loss 1.0496409838755187\n",
      "692-th epoch train loss 1.0127513425064674\n",
      "692-th epoch val loss 1.049641245101389\n",
      "693-th epoch train loss 1.012751917771015\n",
      "693-th epoch val loss 1.0496415022387593\n",
      "694-th epoch train loss 1.012752484011774\n",
      "694-th epoch val loss 1.049641755351363\n",
      "695-th epoch train loss 1.0127530413700434\n",
      "695-th epoch val loss 1.049642004501948\n",
      "696-th epoch train loss 1.012753589984916\n",
      "696-th epoch val loss 1.0496422497522901\n",
      "697-th epoch train loss 1.0127541299933145\n",
      "697-th epoch val loss 1.0496424911632107\n",
      "698-th epoch train loss 1.0127546615300254\n",
      "698-th epoch val loss 1.0496427287945902\n",
      "699-th epoch train loss 1.0127551847277314\n",
      "699-th epoch val loss 1.0496429627053818\n",
      "700-th epoch train loss 1.012755699717043\n",
      "700-th epoch val loss 1.0496431929536254\n",
      "701-th epoch train loss 1.012756206626533\n",
      "701-th epoch val loss 1.049643419596464\n",
      "702-th epoch train loss 1.0127567055827669\n",
      "702-th epoch val loss 1.0496436426901534\n",
      "703-th epoch train loss 1.0127571967103322\n",
      "703-th epoch val loss 1.049643862290079\n",
      "704-th epoch train loss 1.012757680131873\n",
      "704-th epoch val loss 1.0496440784507675\n",
      "705-th epoch train loss 1.012758155968116\n",
      "705-th epoch val loss 1.0496442912259007\n",
      "706-th epoch train loss 1.0127586243379043\n",
      "706-th epoch val loss 1.049644500668329\n",
      "707-th epoch train loss 1.0127590853582222\n",
      "707-th epoch val loss 1.0496447068300814\n",
      "708-th epoch train loss 1.0127595391442288\n",
      "708-th epoch val loss 1.0496449097623812\n",
      "709-th epoch train loss 1.0127599858092822\n",
      "709-th epoch val loss 1.049645109515658\n",
      "710-th epoch train loss 1.0127604254649707\n",
      "710-th epoch val loss 1.049645306139556\n",
      "711-th epoch train loss 1.0127608582211378\n",
      "711-th epoch val loss 1.0496454996829518\n",
      "712-th epoch train loss 1.0127612841859104\n",
      "712-th epoch val loss 1.049645690193962\n",
      "713-th epoch train loss 1.0127617034657268\n",
      "713-th epoch val loss 1.0496458777199573\n",
      "714-th epoch train loss 1.0127621161653597\n",
      "714-th epoch val loss 1.049646062307571\n",
      "715-th epoch train loss 1.0127625223879457\n",
      "715-th epoch val loss 1.0496462440027126\n",
      "716-th epoch train loss 1.0127629222350076\n",
      "716-th epoch val loss 1.0496464228505804\n",
      "717-th epoch train loss 1.012763315806482\n",
      "717-th epoch val loss 1.0496465988956687\n",
      "718-th epoch train loss 1.0127637032007422\n",
      "718-th epoch val loss 1.0496467721817797\n",
      "719-th epoch train loss 1.0127640845146237\n",
      "719-th epoch val loss 1.0496469427520363\n",
      "720-th epoch train loss 1.0127644598434478\n",
      "720-th epoch val loss 1.0496471106488905\n",
      "721-th epoch train loss 1.0127648292810445\n",
      "721-th epoch val loss 1.049647275914134\n",
      "722-th epoch train loss 1.012765192919776\n",
      "722-th epoch val loss 1.0496474385889085\n",
      "723-th epoch train loss 1.012765550850561\n",
      "723-th epoch val loss 1.0496475987137173\n",
      "724-th epoch train loss 1.0127659031628953\n",
      "724-th epoch val loss 1.0496477563284314\n",
      "725-th epoch train loss 1.0127662499448737\n",
      "725-th epoch val loss 1.0496479114723023\n",
      "726-th epoch train loss 1.012766591283214\n",
      "726-th epoch val loss 1.049648064183972\n",
      "727-th epoch train loss 1.012766927263277\n",
      "727-th epoch val loss 1.0496482145014796\n",
      "728-th epoch train loss 1.012767257969088\n",
      "728-th epoch val loss 1.0496483624622726\n",
      "729-th epoch train loss 1.0127675834833563\n",
      "729-th epoch val loss 1.0496485081032145\n",
      "730-th epoch train loss 1.0127679038874977\n",
      "730-th epoch val loss 1.049648651460597\n",
      "731-th epoch train loss 1.0127682192616534\n",
      "731-th epoch val loss 1.0496487925701445\n",
      "732-th epoch train loss 1.0127685296847109\n",
      "732-th epoch val loss 1.049648931467026\n",
      "733-th epoch train loss 1.0127688352343227\n",
      "733-th epoch val loss 1.0496490681858637\n",
      "734-th epoch train loss 1.0127691359869253\n",
      "734-th epoch val loss 1.0496492027607376\n",
      "735-th epoch train loss 1.0127694320177596\n",
      "735-th epoch val loss 1.0496493352251994\n",
      "736-th epoch train loss 1.0127697234008872\n",
      "736-th epoch val loss 1.0496494656122766\n",
      "737-th epoch train loss 1.0127700102092112\n",
      "737-th epoch val loss 1.0496495939544819\n",
      "738-th epoch train loss 1.0127702925144928\n",
      "738-th epoch val loss 1.049649720283822\n",
      "739-th epoch train loss 1.0127705703873695\n",
      "739-th epoch val loss 1.0496498446318037\n",
      "740-th epoch train loss 1.012770843897373\n",
      "740-th epoch val loss 1.0496499670294435\n",
      "741-th epoch train loss 1.0127711131129467\n",
      "741-th epoch val loss 1.0496500875072732\n",
      "742-th epoch train loss 1.0127713781014602\n",
      "742-th epoch val loss 1.049650206095349\n",
      "743-th epoch train loss 1.0127716389292305\n",
      "743-th epoch val loss 1.0496503228232588\n",
      "744-th epoch train loss 1.012771895661534\n",
      "744-th epoch val loss 1.0496504377201286\n",
      "745-th epoch train loss 1.012772148362625\n",
      "745-th epoch val loss 1.0496505508146288\n",
      "746-th epoch train loss 1.0127723970957516\n",
      "746-th epoch val loss 1.049650662134984\n",
      "747-th epoch train loss 1.012772641923172\n",
      "747-th epoch val loss 1.0496507717089782\n",
      "748-th epoch train loss 1.0127728829061668\n",
      "748-th epoch val loss 1.0496508795639607\n",
      "749-th epoch train loss 1.0127731201050587\n",
      "749-th epoch val loss 1.049650985726855\n",
      "750-th epoch train loss 1.012773353579225\n",
      "750-th epoch val loss 1.0496510902241645\n",
      "751-th epoch train loss 1.0127735833871117\n",
      "751-th epoch val loss 1.0496511930819779\n",
      "752-th epoch train loss 1.0127738095862493\n",
      "752-th epoch val loss 1.0496512943259761\n",
      "753-th epoch train loss 1.0127740322332672\n",
      "753-th epoch val loss 1.0496513939814411\n",
      "754-th epoch train loss 1.012774251383908\n",
      "754-th epoch val loss 1.0496514920732585\n",
      "755-th epoch train loss 1.0127744670930392\n",
      "755-th epoch val loss 1.0496515886259254\n",
      "756-th epoch train loss 1.0127746794146701\n",
      "756-th epoch val loss 1.0496516836635574\n",
      "757-th epoch train loss 1.0127748884019618\n",
      "757-th epoch val loss 1.0496517772098908\n",
      "758-th epoch train loss 1.0127750941072442\n",
      "758-th epoch val loss 1.0496518692882946\n",
      "759-th epoch train loss 1.0127752965820254\n",
      "759-th epoch val loss 1.0496519599217695\n",
      "760-th epoch train loss 1.0127754958770072\n",
      "760-th epoch val loss 1.049652049132959\n",
      "761-th epoch train loss 1.012775692042095\n",
      "761-th epoch val loss 1.0496521369441516\n",
      "762-th epoch train loss 1.0127758851264141\n",
      "762-th epoch val loss 1.0496522233772876\n",
      "763-th epoch train loss 1.012776075178319\n",
      "763-th epoch val loss 1.0496523084539646\n",
      "764-th epoch train loss 1.0127762622454053\n",
      "764-th epoch val loss 1.0496523921954428\n",
      "765-th epoch train loss 1.0127764463745237\n",
      "765-th epoch val loss 1.0496524746226503\n",
      "766-th epoch train loss 1.0127766276117882\n",
      "766-th epoch val loss 1.049652555756186\n",
      "767-th epoch train loss 1.0127768060025923\n",
      "767-th epoch val loss 1.0496526356163298\n",
      "768-th epoch train loss 1.0127769815916172\n",
      "768-th epoch val loss 1.0496527142230427\n",
      "769-th epoch train loss 1.0127771544228423\n",
      "769-th epoch val loss 1.0496527915959735\n",
      "770-th epoch train loss 1.0127773245395584\n",
      "770-th epoch val loss 1.0496528677544643\n",
      "771-th epoch train loss 1.0127774919843777\n",
      "771-th epoch val loss 1.049652942717555\n",
      "772-th epoch train loss 1.0127776567992433\n",
      "772-th epoch val loss 1.0496530165039863\n",
      "773-th epoch train loss 1.012777819025441\n",
      "773-th epoch val loss 1.0496530891322071\n",
      "774-th epoch train loss 1.0127779787036093\n",
      "774-th epoch val loss 1.0496531606203772\n",
      "775-th epoch train loss 1.012778135873749\n",
      "775-th epoch val loss 1.0496532309863724\n",
      "776-th epoch train loss 1.0127782905752345\n",
      "776-th epoch val loss 1.0496533002477888\n",
      "777-th epoch train loss 1.012778442846821\n",
      "777-th epoch val loss 1.049653368421947\n",
      "778-th epoch train loss 1.0127785927266575\n",
      "778-th epoch val loss 1.0496534355258968\n",
      "779-th epoch train loss 1.0127787402522932\n",
      "779-th epoch val loss 1.0496535015764212\n",
      "780-th epoch train loss 1.01277888546069\n",
      "780-th epoch val loss 1.0496535665900406\n",
      "781-th epoch train loss 1.012779028388229\n",
      "781-th epoch val loss 1.0496536305830164\n",
      "782-th epoch train loss 1.01277916907072\n",
      "782-th epoch val loss 1.0496536935713563\n",
      "783-th epoch train loss 1.0127793075434126\n",
      "783-th epoch val loss 1.0496537555708167\n",
      "784-th epoch train loss 1.012779443841002\n",
      "784-th epoch val loss 1.0496538165969076\n",
      "785-th epoch train loss 1.01277957799764\n",
      "785-th epoch val loss 1.049653876664896\n",
      "786-th epoch train loss 1.012779710046943\n",
      "786-th epoch val loss 1.04965393578981\n",
      "787-th epoch train loss 1.0127798400219987\n",
      "787-th epoch val loss 1.0496539939864435\n",
      "788-th epoch train loss 1.0127799679553764\n",
      "788-th epoch val loss 1.0496540512693566\n",
      "789-th epoch train loss 1.012780093879135\n",
      "789-th epoch val loss 1.0496541076528834\n",
      "790-th epoch train loss 1.012780217824829\n",
      "790-th epoch val loss 1.0496541631511318\n",
      "791-th epoch train loss 1.0127803398235196\n",
      "791-th epoch val loss 1.0496542177779906\n",
      "792-th epoch train loss 1.012780459905779\n",
      "792-th epoch val loss 1.04965427154713\n",
      "793-th epoch train loss 1.0127805781017003\n",
      "793-th epoch val loss 1.0496543244720056\n",
      "794-th epoch train loss 1.0127806944409052\n",
      "794-th epoch val loss 1.0496543765658635\n",
      "795-th epoch train loss 1.0127808089525494\n",
      "795-th epoch val loss 1.0496544278417417\n",
      "796-th epoch train loss 1.012780921665332\n",
      "796-th epoch val loss 1.0496544783124737\n",
      "797-th epoch train loss 1.0127810326075013\n",
      "797-th epoch val loss 1.0496545279906933\n",
      "798-th epoch train loss 1.012781141806862\n",
      "798-th epoch val loss 1.0496545768888346\n",
      "799-th epoch train loss 1.0127812492907828\n",
      "799-th epoch val loss 1.0496546250191372\n",
      "800-th epoch train loss 1.012781355086203\n",
      "800-th epoch val loss 1.04965467239365\n",
      "801-th epoch train loss 1.0127814592196398\n",
      "801-th epoch val loss 1.049654719024233\n",
      "802-th epoch train loss 1.012781561717193\n",
      "802-th epoch val loss 1.0496547649225592\n",
      "803-th epoch train loss 1.0127816626045527\n",
      "803-th epoch val loss 1.0496548101001189\n",
      "804-th epoch train loss 1.0127817619070068\n",
      "804-th epoch val loss 1.0496548545682234\n",
      "805-th epoch train loss 1.0127818596494462\n",
      "805-th epoch val loss 1.049654898338007\n",
      "806-th epoch train loss 1.0127819558563707\n",
      "806-th epoch val loss 1.0496549414204275\n",
      "807-th epoch train loss 1.0127820505518952\n",
      "807-th epoch val loss 1.049654983826272\n",
      "808-th epoch train loss 1.0127821437597575\n",
      "808-th epoch val loss 1.0496550255661596\n",
      "809-th epoch train loss 1.0127822355033214\n",
      "809-th epoch val loss 1.0496550666505418\n",
      "810-th epoch train loss 1.0127823258055846\n",
      "810-th epoch val loss 1.0496551070897062\n",
      "811-th epoch train loss 1.0127824146891833\n",
      "811-th epoch val loss 1.0496551468937796\n",
      "812-th epoch train loss 1.0127825021763996\n",
      "812-th epoch val loss 1.0496551860727297\n",
      "813-th epoch train loss 1.0127825882891643\n",
      "813-th epoch val loss 1.049655224636368\n",
      "814-th epoch train loss 1.0127826730490643\n",
      "814-th epoch val loss 1.0496552625943527\n",
      "815-th epoch train loss 1.0127827564773495\n",
      "815-th epoch val loss 1.0496552999561908\n",
      "816-th epoch train loss 1.0127828385949333\n",
      "816-th epoch val loss 1.0496553367312387\n",
      "817-th epoch train loss 1.0127829194224027\n",
      "817-th epoch val loss 1.049655372928707\n",
      "818-th epoch train loss 1.012782998980021\n",
      "818-th epoch val loss 1.0496554085576628\n",
      "819-th epoch train loss 1.0127830772877335\n",
      "819-th epoch val loss 1.0496554436270298\n",
      "820-th epoch train loss 1.0127831543651722\n",
      "820-th epoch val loss 1.0496554781455925\n",
      "821-th epoch train loss 1.012783230231661\n",
      "821-th epoch val loss 1.0496555121219966\n",
      "822-th epoch train loss 1.012783304906221\n",
      "822-th epoch val loss 1.0496555455647538\n",
      "823-th epoch train loss 1.0127833784075728\n",
      "823-th epoch val loss 1.0496555784822406\n",
      "824-th epoch train loss 1.0127834507541458\n",
      "824-th epoch val loss 1.049655610882703\n",
      "825-th epoch train loss 1.012783521964077\n",
      "825-th epoch val loss 1.0496556427742585\n",
      "826-th epoch train loss 1.0127835920552217\n",
      "826-th epoch val loss 1.0496556741648966\n",
      "827-th epoch train loss 1.0127836610451535\n",
      "827-th epoch val loss 1.0496557050624804\n",
      "828-th epoch train loss 1.0127837289511687\n",
      "828-th epoch val loss 1.0496557354747509\n",
      "829-th epoch train loss 1.0127837957902943\n",
      "829-th epoch val loss 1.049655765409328\n",
      "830-th epoch train loss 1.0127838615792892\n",
      "830-th epoch val loss 1.0496557948737115\n",
      "831-th epoch train loss 1.0127839263346485\n",
      "831-th epoch val loss 1.049655823875283\n",
      "832-th epoch train loss 1.0127839900726088\n",
      "832-th epoch val loss 1.0496558524213093\n",
      "833-th epoch train loss 1.0127840528091514\n",
      "833-th epoch val loss 1.0496558805189424\n",
      "834-th epoch train loss 1.0127841145600074\n",
      "834-th epoch val loss 1.0496559081752226\n",
      "835-th epoch train loss 1.012784175340661\n",
      "835-th epoch val loss 1.0496559353970802\n",
      "836-th epoch train loss 1.0127842351663514\n",
      "836-th epoch val loss 1.0496559621913353\n",
      "837-th epoch train loss 1.0127842940520806\n",
      "837-th epoch val loss 1.0496559885647023\n",
      "838-th epoch train loss 1.012784352012614\n",
      "838-th epoch val loss 1.0496560145237905\n",
      "839-th epoch train loss 1.012784409062486\n",
      "839-th epoch val loss 1.0496560400751045\n",
      "840-th epoch train loss 1.0127844652160014\n",
      "840-th epoch val loss 1.0496560652250468\n",
      "841-th epoch train loss 1.012784520487241\n",
      "841-th epoch val loss 1.0496560899799203\n",
      "842-th epoch train loss 1.0127845748900652\n",
      "842-th epoch val loss 1.0496561143459286\n",
      "843-th epoch train loss 1.0127846284381161\n",
      "843-th epoch val loss 1.0496561383291778\n",
      "844-th epoch train loss 1.0127846811448218\n",
      "844-th epoch val loss 1.0496561619356792\n",
      "845-th epoch train loss 1.0127847330233986\n",
      "845-th epoch val loss 1.0496561851713475\n",
      "846-th epoch train loss 1.0127847840868558\n",
      "846-th epoch val loss 1.0496562080420067\n",
      "847-th epoch train loss 1.0127848343479995\n",
      "847-th epoch val loss 1.0496562305533887\n",
      "848-th epoch train loss 1.012784883819433\n",
      "848-th epoch val loss 1.0496562527111355\n",
      "849-th epoch train loss 1.0127849325135634\n",
      "849-th epoch val loss 1.049656274520801\n",
      "850-th epoch train loss 1.0127849804426008\n",
      "850-th epoch val loss 1.0496562959878502\n",
      "851-th epoch train loss 1.0127850276185653\n",
      "851-th epoch val loss 1.0496563171176647\n",
      "852-th epoch train loss 1.0127850740532875\n",
      "852-th epoch val loss 1.0496563379155401\n",
      "853-th epoch train loss 1.0127851197584121\n",
      "853-th epoch val loss 1.0496563583866894\n",
      "854-th epoch train loss 1.0127851647454016\n",
      "854-th epoch val loss 1.0496563785362436\n",
      "855-th epoch train loss 1.0127852090255383\n",
      "855-th epoch val loss 1.0496563983692542\n",
      "856-th epoch train loss 1.0127852526099264\n",
      "856-th epoch val loss 1.049656417890691\n",
      "857-th epoch train loss 1.012785295509496\n",
      "857-th epoch val loss 1.049656437105449\n",
      "858-th epoch train loss 1.0127853377350062\n",
      "858-th epoch val loss 1.0496564560183426\n",
      "859-th epoch train loss 1.012785379297047\n",
      "859-th epoch val loss 1.049656474634114\n",
      "860-th epoch train loss 1.012785420206042\n",
      "860-th epoch val loss 1.0496564929574304\n",
      "861-th epoch train loss 1.0127854604722504\n",
      "861-th epoch val loss 1.0496565109928837\n",
      "862-th epoch train loss 1.012785500105771\n",
      "862-th epoch val loss 1.0496565287449955\n",
      "863-th epoch train loss 1.012785539116544\n",
      "863-th epoch val loss 1.049656546218216\n",
      "864-th epoch train loss 1.0127855775143524\n",
      "864-th epoch val loss 1.0496565634169253\n",
      "865-th epoch train loss 1.0127856153088275\n",
      "865-th epoch val loss 1.049656580345435\n",
      "866-th epoch train loss 1.0127856525094476\n",
      "866-th epoch val loss 1.0496565970079892\n",
      "867-th epoch train loss 1.0127856891255422\n",
      "867-th epoch val loss 1.0496566134087646\n",
      "868-th epoch train loss 1.0127857251662953\n",
      "868-th epoch val loss 1.0496566295518732\n",
      "869-th epoch train loss 1.0127857606407464\n",
      "869-th epoch val loss 1.0496566454413623\n",
      "870-th epoch train loss 1.0127857955577921\n",
      "870-th epoch val loss 1.0496566610812152\n",
      "871-th epoch train loss 1.0127858299261907\n",
      "871-th epoch val loss 1.0496566764753528\n",
      "872-th epoch train loss 1.0127858637545613\n",
      "872-th epoch val loss 1.0496566916276353\n",
      "873-th epoch train loss 1.0127858970513886\n",
      "873-th epoch val loss 1.0496567065418607\n",
      "874-th epoch train loss 1.0127859298250244\n",
      "874-th epoch val loss 1.0496567212217687\n",
      "875-th epoch train loss 1.012785962083688\n",
      "875-th epoch val loss 1.0496567356710402\n",
      "876-th epoch train loss 1.0127859938354709\n",
      "876-th epoch val loss 1.049656749893297\n",
      "877-th epoch train loss 1.0127860250883367\n",
      "877-th epoch val loss 1.0496567638921057\n",
      "878-th epoch train loss 1.0127860558501243\n",
      "878-th epoch val loss 1.0496567776709762\n",
      "879-th epoch train loss 1.0127860861285483\n",
      "879-th epoch val loss 1.049656791233362\n",
      "880-th epoch train loss 1.0127861159312048\n",
      "880-th epoch val loss 1.0496568045826657\n",
      "881-th epoch train loss 1.012786145265568\n",
      "881-th epoch val loss 1.0496568177222332\n",
      "882-th epoch train loss 1.0127861741389954\n",
      "882-th epoch val loss 1.0496568306553586\n",
      "883-th epoch train loss 1.0127862025587298\n",
      "883-th epoch val loss 1.0496568433852862\n",
      "884-th epoch train loss 1.0127862305318989\n",
      "884-th epoch val loss 1.0496568559152075\n",
      "885-th epoch train loss 1.0127862580655187\n",
      "885-th epoch val loss 1.0496568682482628\n",
      "886-th epoch train loss 1.0127862851664962\n",
      "886-th epoch val loss 1.0496568803875463\n",
      "887-th epoch train loss 1.012786311841629\n",
      "887-th epoch val loss 1.0496568923361014\n",
      "888-th epoch train loss 1.0127863380976072\n",
      "888-th epoch val loss 1.049656904096924\n",
      "889-th epoch train loss 1.012786363941017\n",
      "889-th epoch val loss 1.0496569156729632\n",
      "890-th epoch train loss 1.0127863893783413\n",
      "890-th epoch val loss 1.0496569270671219\n",
      "891-th epoch train loss 1.0127864144159597\n",
      "891-th epoch val loss 1.0496569382822574\n",
      "892-th epoch train loss 1.0127864390601526\n",
      "892-th epoch val loss 1.0496569493211818\n",
      "893-th epoch train loss 1.012786463317102\n",
      "893-th epoch val loss 1.0496569601866634\n",
      "894-th epoch train loss 1.0127864871928922\n",
      "894-th epoch val loss 1.049656970881427\n",
      "895-th epoch train loss 1.0127865106935112\n",
      "895-th epoch val loss 1.0496569814081536\n",
      "896-th epoch train loss 1.012786533824855\n",
      "896-th epoch val loss 1.0496569917694842\n",
      "897-th epoch train loss 1.012786556592725\n",
      "897-th epoch val loss 1.0496570019680158\n",
      "898-th epoch train loss 1.0127865790028328\n",
      "898-th epoch val loss 1.0496570120063076\n",
      "899-th epoch train loss 1.012786601060799\n",
      "899-th epoch val loss 1.0496570218868753\n",
      "900-th epoch train loss 1.0127866227721571\n",
      "900-th epoch val loss 1.0496570316121971\n",
      "901-th epoch train loss 1.012786644142353\n",
      "901-th epoch val loss 1.0496570411847126\n",
      "902-th epoch train loss 1.0127866651767472\n",
      "902-th epoch val loss 1.049657050606822\n",
      "903-th epoch train loss 1.0127866858806165\n",
      "903-th epoch val loss 1.049657059880888\n",
      "904-th epoch train loss 1.0127867062591536\n",
      "904-th epoch val loss 1.0496570690092368\n",
      "905-th epoch train loss 1.0127867263174701\n",
      "905-th epoch val loss 1.0496570779941565\n",
      "906-th epoch train loss 1.0127867460605986\n",
      "906-th epoch val loss 1.0496570868379023\n",
      "907-th epoch train loss 1.0127867654934906\n",
      "907-th epoch val loss 1.0496570955426903\n",
      "908-th epoch train loss 1.0127867846210208\n",
      "908-th epoch val loss 1.0496571041107046\n",
      "909-th epoch train loss 1.0127868034479874\n",
      "909-th epoch val loss 1.0496571125440932\n",
      "910-th epoch train loss 1.0127868219791138\n",
      "910-th epoch val loss 1.0496571208449723\n",
      "911-th epoch train loss 1.0127868402190472\n",
      "911-th epoch val loss 1.0496571290154229\n",
      "912-th epoch train loss 1.0127868581723645\n",
      "912-th epoch val loss 1.0496571370574945\n",
      "913-th epoch train loss 1.0127868758435676\n",
      "913-th epoch val loss 1.0496571449732033\n",
      "914-th epoch train loss 1.0127868932370903\n",
      "914-th epoch val loss 1.0496571527645349\n",
      "915-th epoch train loss 1.0127869103572962\n",
      "915-th epoch val loss 1.049657160433444\n",
      "916-th epoch train loss 1.0127869272084788\n",
      "916-th epoch val loss 1.049657167981853\n",
      "917-th epoch train loss 1.0127869437948656\n",
      "917-th epoch val loss 1.0496571754116555\n",
      "918-th epoch train loss 1.0127869601206179\n",
      "918-th epoch val loss 1.049657182724715\n",
      "919-th epoch train loss 1.0127869761898307\n",
      "919-th epoch val loss 1.049657189922866\n",
      "920-th epoch train loss 1.0127869920065349\n",
      "920-th epoch val loss 1.0496571970079127\n",
      "921-th epoch train loss 1.012787007574698\n",
      "921-th epoch val loss 1.0496572039816334\n",
      "922-th epoch train loss 1.0127870228982256\n",
      "922-th epoch val loss 1.0496572108457765\n",
      "923-th epoch train loss 1.012787037980962\n",
      "923-th epoch val loss 1.0496572176020638\n",
      "924-th epoch train loss 1.01278705282669\n",
      "924-th epoch val loss 1.0496572242521895\n",
      "925-th epoch train loss 1.012787067439134\n",
      "925-th epoch val loss 1.0496572307978216\n",
      "926-th epoch train loss 1.0127870818219602\n",
      "926-th epoch val loss 1.0496572372406023\n",
      "927-th epoch train loss 1.0127870959787761\n",
      "927-th epoch val loss 1.0496572435821474\n",
      "928-th epoch train loss 1.0127871099131334\n",
      "928-th epoch val loss 1.0496572498240475\n",
      "929-th epoch train loss 1.0127871236285273\n",
      "929-th epoch val loss 1.0496572559678683\n",
      "930-th epoch train loss 1.012787137128398\n",
      "930-th epoch val loss 1.0496572620151494\n",
      "931-th epoch train loss 1.0127871504161332\n",
      "931-th epoch val loss 1.0496572679674099\n",
      "932-th epoch train loss 1.0127871634950658\n",
      "932-th epoch val loss 1.049657273826141\n",
      "933-th epoch train loss 1.0127871763684761\n",
      "933-th epoch val loss 1.0496572795928127\n",
      "934-th epoch train loss 1.0127871890395943\n",
      "934-th epoch val loss 1.0496572852688715\n",
      "935-th epoch train loss 1.0127872015115986\n",
      "935-th epoch val loss 1.049657290855741\n",
      "936-th epoch train loss 1.0127872137876182\n",
      "936-th epoch val loss 1.049657296354823\n",
      "937-th epoch train loss 1.0127872258707324\n",
      "937-th epoch val loss 1.0496573017674962\n",
      "938-th epoch train loss 1.0127872377639724\n",
      "938-th epoch val loss 1.0496573070951183\n",
      "939-th epoch train loss 1.012787249470322\n",
      "939-th epoch val loss 1.0496573123390265\n",
      "940-th epoch train loss 1.012787260992718\n",
      "940-th epoch val loss 1.0496573175005346\n",
      "941-th epoch train loss 1.0127872723340503\n",
      "941-th epoch val loss 1.049657322580938\n",
      "942-th epoch train loss 1.0127872834971643\n",
      "942-th epoch val loss 1.0496573275815115\n",
      "943-th epoch train loss 1.0127872944848606\n",
      "943-th epoch val loss 1.049657332503509\n",
      "944-th epoch train loss 1.012787305299896\n",
      "944-th epoch val loss 1.049657337348165\n",
      "945-th epoch train loss 1.0127873159449832\n",
      "945-th epoch val loss 1.0496573421166955\n",
      "946-th epoch train loss 1.012787326422792\n",
      "946-th epoch val loss 1.0496573468102952\n",
      "947-th epoch train loss 1.0127873367359526\n",
      "947-th epoch val loss 1.0496573514301426\n",
      "948-th epoch train loss 1.0127873468870507\n",
      "948-th epoch val loss 1.0496573559773958\n",
      "949-th epoch train loss 1.0127873568786336\n",
      "949-th epoch val loss 1.0496573604531962\n",
      "950-th epoch train loss 1.012787366713208\n",
      "950-th epoch val loss 1.0496573648586656\n",
      "951-th epoch train loss 1.01278737639324\n",
      "951-th epoch val loss 1.0496573691949094\n",
      "952-th epoch train loss 1.012787385921159\n",
      "952-th epoch val loss 1.0496573734630157\n",
      "953-th epoch train loss 1.0127873952993551\n",
      "953-th epoch val loss 1.0496573776640545\n",
      "954-th epoch train loss 1.0127874045301812\n",
      "954-th epoch val loss 1.0496573817990802\n",
      "955-th epoch train loss 1.0127874136159523\n",
      "955-th epoch val loss 1.0496573858691294\n",
      "956-th epoch train loss 1.0127874225589484\n",
      "956-th epoch val loss 1.0496573898752237\n",
      "957-th epoch train loss 1.012787431361413\n",
      "957-th epoch val loss 1.0496573938183675\n",
      "958-th epoch train loss 1.0127874400255539\n",
      "958-th epoch val loss 1.0496573976995502\n",
      "959-th epoch train loss 1.0127874485535453\n",
      "959-th epoch val loss 1.0496574015197453\n",
      "960-th epoch train loss 1.0127874569475264\n",
      "960-th epoch val loss 1.0496574052799112\n",
      "961-th epoch train loss 1.0127874652096023\n",
      "961-th epoch val loss 1.0496574089809905\n",
      "962-th epoch train loss 1.012787473341847\n",
      "962-th epoch val loss 1.049657412623913\n",
      "963-th epoch train loss 1.0127874813463\n",
      "963-th epoch val loss 1.0496574162095913\n",
      "964-th epoch train loss 1.0127874892249693\n",
      "964-th epoch val loss 1.0496574197389255\n",
      "965-th epoch train loss 1.0127874969798314\n",
      "965-th epoch val loss 1.0496574232127998\n",
      "966-th epoch train loss 1.0127875046128318\n",
      "966-th epoch val loss 1.0496574266320873\n",
      "967-th epoch train loss 1.0127875121258862\n",
      "967-th epoch val loss 1.0496574299976447\n",
      "968-th epoch train loss 1.0127875195208782\n",
      "968-th epoch val loss 1.0496574333103164\n",
      "969-th epoch train loss 1.0127875267996633\n",
      "969-th epoch val loss 1.0496574365709332\n",
      "970-th epoch train loss 1.0127875339640684\n",
      "970-th epoch val loss 1.0496574397803133\n",
      "971-th epoch train loss 1.0127875410158893\n",
      "971-th epoch val loss 1.0496574429392618\n",
      "972-th epoch train loss 1.012787547956897\n",
      "972-th epoch val loss 1.0496574460485713\n",
      "973-th epoch train loss 1.0127875547888319\n",
      "973-th epoch val loss 1.0496574491090211\n",
      "974-th epoch train loss 1.0127875615134077\n",
      "974-th epoch val loss 1.0496574521213797\n",
      "975-th epoch train loss 1.0127875681323117\n",
      "975-th epoch val loss 1.049657455086403\n",
      "976-th epoch train loss 1.0127875746472048\n",
      "976-th epoch val loss 1.049657458004834\n",
      "977-th epoch train loss 1.0127875810597207\n",
      "977-th epoch val loss 1.049657460877405\n",
      "978-th epoch train loss 1.0127875873714685\n",
      "978-th epoch val loss 1.0496574637048364\n",
      "979-th epoch train loss 1.012787593584032\n",
      "979-th epoch val loss 1.049657466487838\n",
      "980-th epoch train loss 1.0127875996989688\n",
      "980-th epoch val loss 1.0496574692271075\n",
      "981-th epoch train loss 1.0127876057178145\n",
      "981-th epoch val loss 1.049657471923333\n",
      "982-th epoch train loss 1.0127876116420775\n",
      "982-th epoch val loss 1.049657474577189\n",
      "983-th epoch train loss 1.012787617473245\n",
      "983-th epoch val loss 1.0496574771893434\n",
      "984-th epoch train loss 1.01278762321278\n",
      "984-th epoch val loss 1.04965747976045\n",
      "985-th epoch train loss 1.0127876288621218\n",
      "985-th epoch val loss 1.0496574822911544\n",
      "986-th epoch train loss 1.0127876344226883\n",
      "986-th epoch val loss 1.0496574847820914\n",
      "987-th epoch train loss 1.012787639895874\n",
      "987-th epoch val loss 1.0496574872338855\n",
      "988-th epoch train loss 1.0127876452830524\n",
      "988-th epoch val loss 1.0496574896471527\n",
      "989-th epoch train loss 1.0127876505855753\n",
      "989-th epoch val loss 1.0496574920224977\n",
      "990-th epoch train loss 1.0127876558047726\n",
      "990-th epoch val loss 1.0496574943605161\n",
      "991-th epoch train loss 1.0127876609419528\n",
      "991-th epoch val loss 1.049657496661795\n",
      "992-th epoch train loss 1.0127876659984065\n",
      "992-th epoch val loss 1.049657498926912\n",
      "993-th epoch train loss 1.0127876709754005\n",
      "993-th epoch val loss 1.049657501156434\n",
      "994-th epoch train loss 1.0127876758741847\n",
      "994-th epoch val loss 1.0496575033509217\n",
      "995-th epoch train loss 1.0127876806959875\n",
      "995-th epoch val loss 1.0496575055109243\n",
      "996-th epoch train loss 1.0127876854420188\n",
      "996-th epoch val loss 1.049657507636985\n",
      "997-th epoch train loss 1.012787690113469\n",
      "997-th epoch val loss 1.0496575097296363\n",
      "998-th epoch train loss 1.0127876947115102\n",
      "998-th epoch val loss 1.0496575117894038\n",
      "999-th epoch train loss 1.0127876992372966\n",
      "999-th epoch val loss 1.0496575138168038\n"
     ]
    }
   ],
   "source": [
    "slr = ScratchLinearRegression(num_iter=1000, lr=0.01, no_bias=True, verbose=True)\n",
    "slr.fit(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94121a29-2386-4360-a312-046bf9a1e532",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x2a8b62f53a0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(slr.loss)\n",
    "plt.plot(slr.val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70688c3b-96a4-4fbd-8fd5-f7d4bf87b37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = np.mean((y-y_pred)**2)\n",
    "    \n",
    "    return mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1faa2528-39a1-4c02-9c9c-9eaf4c37eb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = slr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ba9cdf6f-4afa-482d-ab70-6f062d1b911e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56196135521.05748"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MSE(np.exp(pred_test), np.exp(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc01ea8c-b1db-4313-a883-209c37fe2417",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
